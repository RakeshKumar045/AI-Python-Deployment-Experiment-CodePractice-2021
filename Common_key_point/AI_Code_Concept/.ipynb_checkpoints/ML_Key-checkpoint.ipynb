{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Set\n",
    "\n",
    "pd.options.display.max_columns = 250\n",
    "pd.options.display.max_rows = 2000\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym # why use gym ?????\n",
    "\n",
    "from scipy import stats as s\n",
    "from random import randint\n",
    "from re import search\n",
    "from pathlib import Path\n",
    "\n",
    "# import pprint\n",
    "from pprint import pprint \n",
    "import re, pprint, random, requests, glob\n",
    "\n",
    "# from pandas_profiling import ProfileReport\n",
    "\n",
    "#ProfileReport(df) # get all details like : min, max, correlation, else\n",
    "\n",
    "# %matplotlib notebook vs %matplotlib inline #vvi : %matplotlib notebook\n",
    "\n",
    "# df=pd.read_csv(\"CarPrice_Assignment.csv\",encoding = \"ISO-8859-1\",low_memory=False)\n",
    "\n",
    " #************ titanic_df.profile_report() #VVI \n",
    "    \n",
    "import sys\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "from timeit import timeit\n",
    "from scipy.stats import mstats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "from collections import Counter , defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "from pandas import Series as s , DataFrame as df\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt, rcParams as rc\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "rc[\"figure.figsize\"] = 10,6\n",
    "\n",
    "\n",
    "#Data Science and ML\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.feature_selection import RFE #Recursive Feature Selection\n",
    "import statsmodels.api as sm \n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, normalize, LabelEncoder\n",
    "\n",
    "# GridSearchCV to find optimal min_samples_leaf, Model Selection\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split, StratifiedKFold, cross_val_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor # why?\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor , XGBRFRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='orange' > Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algorithm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "#Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, recall_score, precision_score, explained_variance_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='blue' > HighLevel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from imblearn.combine import SMOTETomek # over sampling method 1\n",
    "\n",
    "## RandomOverSampler to handle imbalanced data\n",
    "from imblearn.over_sampling import RandomOverSampler # over sampling method 2\n",
    "\n",
    "\n",
    "#Shuffle the dataframe\n",
    "# from sklearn.utils import shuffle # df1 = shuffle(df).reset_index(drop = True)\n",
    "\n",
    "from imblearn.under_sampling import NearMiss\n",
    "\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from pickle import dump, load\n",
    "from joblib import dump, load\n",
    "\n",
    "import datetime\n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' >5) Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[]\n",
    "def addRandomStateForAlgorithm(x,y,names,algorithms,columns_name,random_state_list):    \n",
    "    for j in range(len(algorithms)):\n",
    "        model = algorithms[j]\n",
    "        for i in random_state_list:\n",
    "            x_train, x_test , y_train , y_test = train_test_split(x ,y , test_size = 0.25 , random_state = i)\n",
    "            model.fit(x_train,y_train)\n",
    "            pred_test = model.predict(x_test)\n",
    "            row = [names[j],i,r2_score (y_test,pred_test)]\n",
    "            rows.append(row)\n",
    "    models_df = pd.DataFrame(rows)   \n",
    "    models_df.columns = columns_name\n",
    "    print(models_df)\n",
    "    \n",
    "    \n",
    "def mean_absolute_percentage_error(y_true, y_pred) :\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    return np.mean(np.abs( (y_true - y_pred) / 100) ) * 100\n",
    "\n",
    "\n",
    "def root_mean_sequare_error(y_true, y_pred) : \n",
    "    mse = mean_squared_error(y_true,  y_pred)  \n",
    "    return np.sqrt(mse)\n",
    "   \n",
    "\n",
    "rows=[]\n",
    "\n",
    "def addRandomStateForAlgorithm(x,y,names,algorithms,columns_name,random_state_list):    \n",
    "    for j in range(len(algorithms)):\n",
    "        model = algorithms[j]\n",
    "        for i in random_state_list:\n",
    "            \n",
    "            x_train, x_test , y_train , y_test = train_test_split(x ,y , test_size = 0.30 , random_state = i)\n",
    "            \n",
    "            model.fit(x_train,y_train)\n",
    "            \n",
    "            y_pred_train = model.predict(x_train)\n",
    "            y_pred = model.predict(x_test)\n",
    "            \n",
    "            train_acc = r2_score(y_train, y_pred_train)\n",
    "            train_acc = round(train_acc, 2) * 100\n",
    "            \n",
    "            test_acc = r2_score(y_test, y_pred)\n",
    "            test_acc = round(test_acc, 2) * 100\n",
    "            \n",
    "            mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "            mape = round(mape, 2)\n",
    "            \n",
    "            mae = mean_absolute_error(y_test, y_pred)   \n",
    "            mae = round(mae, 2)\n",
    "            \n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            mse = round(mse, 2)\n",
    "            \n",
    "            rmse = root_mean_sequare_error(y_test, y_pred)\n",
    "            rmse = round(rmse, 2)\n",
    "\n",
    "            row = [names[j],   i,   train_acc, test_acc,  mae,    mse]\n",
    "    \n",
    "            rows.append(row)\n",
    "            \n",
    "    models_df = pd.DataFrame(rows) \n",
    "    \n",
    "    models_df.columns = columns_name\n",
    "    print(models_df)\n",
    "    \n",
    "names_regression = [ \"Lasso\", \"Ridge\"]\n",
    "algorithms = [ Lasso(), Ridge(alpha=ridge_alpha)]\n",
    "\n",
    "columns_name = [\"Model\",    \"Random_state\",   'Train_acc',     \"Test_acc\",   \"MAE\",   \"MSE\"]\n",
    "\n",
    "random_state_list_up_to_10 = [0,1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "\n",
    "addRandomStateForAlgorithm(X,y,names_regression,algorithms,columns_name,random_state_list_up_to_10)\n",
    "\n",
    "\n",
    "def get_accuracy(y_actual, y_pred, data_type = \"Train\"):\n",
    "    print(\"Model Evaluation Dataset Type is  : \", data_type)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    print(\"Accuracy  :  \", r2_score(y_actual, y_pred))\n",
    "    print(\"MAE       :  \", mean_absolute_error(y_actual, y_pred))\n",
    "    print(\"MSLE      :  \", mean_squared_log_error(y_actual, y_pred))\n",
    "    print(\"MSE       :  \", mean_squared_error(y_actual, y_pred))\n",
    "    print(\"RMSE      :  \", np.sqrt(mean_squared_error(y_actual, y_pred)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > 6) Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[]\n",
    "\n",
    "def addRandomStateForAlgorithm(x,y,names,algorithms,columns_name,random_state_list):    \n",
    "    for j in range(len(algorithms)):\n",
    "        model = algorithms[j]\n",
    "        for i in random_state_list:\n",
    "            \n",
    "            x_train, x_test , y_train , y_test = train_test_split(x ,y , test_size = 0.30 , random_state = i)\n",
    "            \n",
    "            model.fit(x_train,y_train)\n",
    "            \n",
    "            y_pred_train = model.predict(x_train)\n",
    "            y_pred = model.predict(x_test)\n",
    "            \n",
    "            train_acc = accuracy_score(y_train, y_pred_train)\n",
    "            train_acc = round(train_acc, 4) * 100\n",
    "            \n",
    "            test_acc = accuracy_score(y_test, y_pred)\n",
    "            test_acc = round(test_acc, 4) * 100\n",
    "            \n",
    "            roc_auc_score_acc = roc_auc_score(y_test, y_pred)\n",
    "            roc_auc_score_acc = round(roc_auc_score_acc, 4) * 100\n",
    "            \n",
    "            row = [names[j],   i,   train_acc, test_acc, roc_auc_score_acc]\n",
    "    \n",
    "            rows.append(row)\n",
    "            \n",
    "    models_df = pd.DataFrame(rows) \n",
    "    \n",
    "    models_df.columns = columns_name\n",
    "    print(models_df)\n",
    "    \n",
    "    \n",
    "    \n",
    "names = [ \"LightGBM\", \"RF\", \"XGBoost\" , \"SVM\", \"LogisticRegression\", \"DTClassifier\", \"CatBoostClassifier\"]\n",
    "\n",
    "algorithms = [ LGBMClassifier(  ), RandomForestClassifier(), XGBClassifier(), SVC() ,\n",
    "              LogisticRegression(), DecisionTreeClassifier(), CatBoostClassifier()]\n",
    "\n",
    "\n",
    "columns_name = [\"Model\",    \"Random_state\",   'Train_acc',     \"Test_acc\" , \"roc_auc_acc\"]\n",
    "\n",
    "random_state_list_up_to_3 = [1,2,3]\n",
    "\n",
    "addRandomStateForAlgorithm(X4,Y4,names,algorithms,columns_name,random_state_list_up_to_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green' > 6.1) Classification (Multi classification or more than 2 o/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier # any classification alg : Ex: LogisticR or DT\n",
    "\n",
    "clf = OneVsOneClassifier(RandomForestClassifier())\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)\n",
    "\n",
    "y_preds = clf.predict(X_test)\n",
    "print(\"Accuracy Score: \",accuracy_score(y_test,y_preds))\n",
    "print(confusion_matrix(y_test,y_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > 7) Cross validation and Model selection , KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, random_state=108)\n",
    "# X is the feature set and y is the target\n",
    "for train_index, test_index in skf.split(X,y): \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    model = CatBoostClassifier(verbose=0)\n",
    "    model.fit(X_train,y_train)\n",
    "    print(model.score(X_test, y_test))\n",
    "    models.append(model)\n",
    "\n",
    "\n",
    "#plz check for regression also? check for train dataset also?\n",
    "def check_cross_validation_overfit(X_val, Y_val, classification_model):\n",
    "    #cv = use diff\n",
    "    cross_validation_accuracy = cross_val_score(estimator = classification_model, X = X_val, y = Y_val, cv = 2).mean()\n",
    "    \n",
    "    model_accuracy =  accuracy_score(X_val,   Y_val)\n",
    "    \n",
    "    print(\"checking overfit & underfit :  almost same\" )\n",
    "\n",
    "    print(\"Model Accuracy : \", model_accuracy, \" & cross validation \", cross_validation_accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='green' > 8) XGBoost Classification with Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_param_test = {\n",
    "    \n",
    "    'gamma': [2, 5,7],\n",
    "    'max_depth': [3, 4,5],\n",
    "    'learning_rate': [0.05,0.07],\n",
    "    'n_estimators': [100,200],\n",
    "#     objective = ['binary:logistic',  'multi:softmax'],\n",
    "#     num_classes = [2, 3] # output : binary(2) or multi classification\n",
    "  \n",
    "}\n",
    "\n",
    "XGB_hyper_params = GridSearchCV(estimator = \n",
    "XGBClassifier(learning_rate =0.1,\n",
    "#               objective= 'binary:logistic',\n",
    "              objective = 'multi:softmax',\n",
    "                                num_classes =  3,\n",
    "              nthread=4,\n",
    "              seed=27), \n",
    "                                \n",
    "              param_grid = xg_param_test,\n",
    "              scoring= 'accuracy',\n",
    "              n_jobs=4,\n",
    "              iid=False,\n",
    "              verbose=10)\n",
    "\n",
    "XGB_hyper_params.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(XGB_hyper_params.best_params_)\n",
    "\n",
    "\n",
    "xgbclf=XGBClassifier(gamma= 7,learning_rate= 0.05, max_depth= 5,n_estimators= 200)\n",
    "xgbclf.fit(X,Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_final = XGBClassifier(\n",
    "#  learning_rate =0.1,\n",
    "#  n_estimators=1000,\n",
    "#  max_depth=5,\n",
    "#  min_child_weight=1,\n",
    "#  gamma=0,\n",
    "#  subsample=0.8,\n",
    "#  colsample_bytree=0.8,\n",
    "#  objective= 'multi:softmax',\n",
    "#  nthread=4,\n",
    "#  scale_pos_weight=1,\n",
    "#  num_classes=3,\n",
    "#  seed=27)\n",
    "\n",
    "\n",
    "\n",
    "model_final = XGBClassifier(max_depth=5, objective='multi:softmax', num_classes=3)\n",
    "\n",
    "model_final = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'binary:logistic',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "\n",
    "model_final.fit(X, Y)\n",
    "\n",
    "y_pred_final = model_final.predict(df5_test)\n",
    "\n",
    "\n",
    "#3rd solution\n",
    "params = {\"objective\": \"multi:softmax\",\"booster\": \"gbtree\", \"nthread\": 4, \"silent\": 1,\n",
    "                \"eta\": 0.08, \"max_depth\": 6, \"subsample\": 0.9, \"colsample_bytree\": 0.7,\n",
    "                \"min_child_weight\": 1, \"num_class\": 3,\n",
    "                \"seed\": 2016, \"tree_method\": \"exact\"}\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, y_train, missing=np.nan)\n",
    "dtest = xgb.DMatrix(X_test, missing=np.nan)\n",
    "\n",
    "nrounds = 260\n",
    "watchlist = [(dtrain, 'train')]\n",
    "bst = xgb.train(params, dtrain, num_boost_round=nrounds, evals=watchlist, verbose_eval=20)\n",
    "test_preds = bst.predict(dtest)\n",
    "\n",
    "#4th\n",
    "params = {\"objective\": \"multi:softmax\",\"booster\": \"gbtree\", \"nthread\": 4, \"silent\": 1,\n",
    "                \"eta\": 0.08, \"max_depth\": 6, \"subsample\": 0.9, \"colsample_bytree\": 0.7,\n",
    "                \"min_child_weight\": 1, \"num_class\": 3,\n",
    "                \"seed\": 2016, \"tree_method\": \"exact\"}\n",
    "dtrain = xgb.DMatrix(X_train, y_train, missing=np.nan)\n",
    "dtest = xgb.DMatrix(X_test, mising=np.nan)\n",
    "\n",
    "nrounds = 260\n",
    "watchlist = [(dtrain, 'train')]\n",
    "bst = xgb.train(params, dtrain, num_boost_round=nrounds, evals=watchlist, verbose_eval=20)\n",
    "test_preds = bst.predict(dtest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='green' > 9) Grid Search Regression with Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "\n",
    "# list of alphas to tune\n",
    "params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n",
    " 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n",
    " 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n",
    "\n",
    "# cross validation\n",
    "folds = 5\n",
    "\n",
    "# cross validation\n",
    "model_cv = GridSearchCV(estimator = lasso, \n",
    "                        param_grid = params, \n",
    "                        scoring= 'neg_mean_absolute_error', \n",
    "                        cv = folds, \n",
    "                        return_train_score=True,\n",
    "                        verbose = 1)            \n",
    "\n",
    "model_cv.fit(X_train, y_train) \n",
    "\n",
    "\n",
    "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
    "cv_results.head()\n",
    "\n",
    "\n",
    "\n",
    "# plotting mean test and train scoes with alpha \n",
    "cv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n",
    "\n",
    "# plotting\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
    "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Negative Mean Absolute Error')\n",
    "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
    "plt.legend(['train score', 'test score'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='green' > 10) LightBoost Classification with hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-cacd6f5ea648>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-cacd6f5ea648>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    param_grid = lg_param_test,\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "lg_param_test = {\n",
    "    \n",
    "    'gamma': [0.5, 0.25],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.07, 0.05],\n",
    "    'n_estimators': [200, 100]\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "lgb_model = LGBMClassifier(learning_rate =0.1, nthread=4,seed=27)\n",
    "\n",
    "LGB_hyper_params = GridSearchCV(estimator = lgb_model\n",
    "              param_grid = lg_param_test,\n",
    "              scoring= 'accuracy',\n",
    "              n_jobs=4,\n",
    "              iid=False,\n",
    "              verbose=10)\n",
    "\n",
    "LGB_hyper_params.fit(X,Y)\n",
    "\n",
    "\n",
    "print(LGB_hyper_params.best_params_)\n",
    "\n",
    "\n",
    "# lgbclf= LGBMClassifier(gamma= 0.5, learning_rate= 0.07,max_depth= 5, n_estimators= 200)\n",
    "# Default  objective='multi:softmax', num_classes=3\n",
    "\n",
    "#or\n",
    "lgbclf= LGBMClassifier(gamma= 0.5, learning_rate= 0.07,max_depth= 5, n_estimators= 200, \n",
    "                       objective='multi:softmax', num_classes=3)\n",
    "\n",
    "\n",
    "lgbclf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model = LGBMClassifier(subsample= 0.9,\n",
    " reg_lambda= 2.5,\n",
    " reg_alpha= 1,\n",
    " random_state= 108,\n",
    " objective= 'multiclass',\n",
    " n_estimators= 4000,\n",
    " min_split_gain= 0.5,\n",
    " min_data_in_leaf= 20,\n",
    " metric= 'multi_logloss',\n",
    " max_depth= 8,\n",
    " learning_rate= 0.01,\n",
    " colsample_bytree= 1,\n",
    " boosting_type= 'gbdt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X ,Y , test_size = 0.05 , random_state = 10)\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "\n",
    "params = {      'bagging_fraction': 0.9, #increased accuracy\n",
    "                'objective':'binary',\n",
    "                'boosting_type':'gbdt',\n",
    "                'metric':'auc',\n",
    "                'n_jobs':-1,\n",
    "                'learning_rate':0.01,\n",
    "                'num_leaves': 2**8,\n",
    "                'max_depth':-1,\n",
    "                'metric': {'l2', 'l1'},\n",
    "                'tree_learner':'serial',\n",
    "                'colsample_bytree': 0.85,\n",
    "                'subsample_freq':1,\n",
    "                'subsample':0.85,\n",
    "                'n_estimators':2**9,\n",
    "                'max_bin':255,\n",
    "                'verbose':-1,\n",
    "                'seed': 42,\n",
    "                'early_stopping_rounds':100,\n",
    "                'reg_alpha':0.3,\n",
    "                'reg_lamdba':0.243\n",
    "            } \n",
    "\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "y_pred_final = gbm.predict(df5_test, num_iteration=gbm.best_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' >11) LightBoost Regression with hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': {'l2', 'l1'},\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "print('Starting training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=20,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "print('Saving model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Starting predicting...')\n",
    "# predict\n",
    "y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "# eval\n",
    "print('The rmse of prediction is:', mean_squared_error(y_test, y_pred) ** 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > 12) CatBoost Classification with hyperparameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_param_test = {\n",
    "    'depth':[2, 3],\n",
    "    'learning_rate': [0.07, 0.05],\n",
    "    'n_estimators': [200, 100],\n",
    "    'loss_function': ['Logloss', 'CrossEntropy', 'MultiClass'],\n",
    "    'l2_leaf_reg':np.logspace(-25, -20, 3)\n",
    "  \n",
    "}\n",
    "\n",
    "CAT_hyper_params = GridSearchCV(estimator = \n",
    "CatBoostClassifier(learning_rate =0.1), \n",
    "              param_grid = cat_param_test,\n",
    "              scoring= 'accuracy',\n",
    "              n_jobs=4,\n",
    "                                \n",
    "              iid=False,\n",
    "              verbose=10)\n",
    "\n",
    "%%time\n",
    "\n",
    "CAT_hyper_params.fit(X,Y)\n",
    "\n",
    "print(CAT_hyper_params.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catclf= CatBoostClassifier(depth= 3,l2_leaf_reg= 1e-25,learning_rate= 0.07,loss_function= 'MultiClass',\n",
    "                           n_estimators= 200)\n",
    "\n",
    "catclf.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_m = CatBoostClassifier(n_estimators=5000,random_state=1994,eval_metric='MultiClass',learning_rate=0.1, max_depth=5)\n",
    "\n",
    "cat_m.fit(X_train, y_train,eval_set=[(X_test, y_test)],early_stopping_rounds=200,erbose=200)\n",
    "\n",
    "y_pred = cat_m.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > 13) PCA : Principle Component Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(svd_solver='randomized', random_state=42) \n",
    "# or\n",
    "# pca = PCA(n_components =54 ) # no of component 54\n",
    "\n",
    "#Doing the PCA on the train data\n",
    "pca.fit(X_train)\n",
    "\n",
    "\n",
    "pca.components_\n",
    "\n",
    "colnames = list(X.columns)\n",
    "pcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\n",
    "pcs_df.head()\n",
    "\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()\n",
    "\n",
    "#final code for pca\n",
    "# Will take the most important 54 features\n",
    "\n",
    "pca_final = PCA(n_components =54 )\n",
    "df_train_pca = pca_final.fit_transform(X_train)\n",
    "df_train_pca.shape\n",
    "\n",
    "#Applying selected components to the test data\n",
    "df_test_pca = pca_final.transform(X_test)\n",
    "df_test_pca.shape\n",
    "\n",
    "#graph\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "plt.plot(np.cumsum(pca_final.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "learner_pca = LogisticRegression()\n",
    "learner_pca.fit(df_train_pca,y_train)\n",
    "\n",
    "y_pred = learner_pca.predict(X_test)\n",
    "accuracy(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_the_model(y_true, y_pred, y_pred_prob):\n",
    "    accuracy_scr = accuracy_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_prob[:, 1])\n",
    "    return accuracy_scr, roc_auc\n",
    "\n",
    "train_pred_lbl = model_code.predict(X_train)\n",
    "train_pred_proba = model_code.predict_proba(X_train)\n",
    "train_accuracy_scr, train_roc_auc = evaluate_the_model(Y_train, train_pred_lbl, train_pred_proba)\n",
    "\n",
    "\n",
    "def get_models_data(models, X_train, Y_train, X_test, Y_test, get_train_score = False):\n",
    "    trained_model = {}\n",
    "    for model_name, model_code in models.items():\n",
    "        print(f\"Model code: {model_code}\")\n",
    "        model_code.fit(X_train, Y_train)\n",
    "        if get_train_score:\n",
    "            train_pred_lbl = model_code.predict(X_train)\n",
    "            train_pred_proba = model_code.predict_proba(X_train)\n",
    "            train_accuracy_scr, train_roc_auc = evaluate_the_model(Y_train, train_pred_lbl, train_pred_proba)\n",
    "            print(f\"Train Accuracy for the model is: {train_accuracy_scr:.2f}\")\n",
    "            print(f\"Train roc_auc_score for the model is: {train_accuracy_scr:.2f}\")\n",
    "            print(f\"{'-'*40}\")\n",
    "            \n",
    "        pred_lbl = model_code.predict(X_test)\n",
    "        pred_proba = model_code.predict_proba(X_test)\n",
    "        accuracy_scr, roc_auc = evaluate_the_model(Y_test, pred_lbl, pred_proba)\n",
    "        print(f\"Test Accuracy for the model is: {accuracy_scr:.2f}\")\n",
    "        print(f\"Test roc_auc_score for the model is: {roc_auc:.2f}\")\n",
    "        print(f\"{'*'*60}\")\n",
    "        trained_model[model_name] = model_code\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "\n",
    "models = {\n",
    "\"logistic_regression\": LogisticRegression(max_iter=500),\n",
    "\"decision_tree_gini\": DecisionTreeClassifier(criterion=\"gini\"),\n",
    "\"decision_tree_entropy\": DecisionTreeClassifier(criterion=\"entropy\"),\n",
    "\"gradint_boost_mse\": GradientBoostingClassifier(),\n",
    "\"rf\": RandomForestClassifier(),\n",
    "\"xgboost\": XGBClassifier(objective= 'binary:logistic'),\n",
    "\"light_gbm\": LGBMClassifier(objective='binary', class_weight='balanced')\n",
    "}\n",
    "\n",
    "trained_models = get_models_data(models, X_train, Y_train, X_test, Y_test, get_train_score=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_parameters_hypertune(model, param_dict, x_train, y_train, nt=100):\n",
    "    print(f\"Model passed: {model}\")\n",
    "    \n",
    "    cv = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "    \n",
    "    random_search_obj = RandomizedSearchCV(estimator = model, param_distributions = param_dict, n_iter = nt, \\\n",
    "                                           cv = cv,verbose = 100,random_state = 10,scoring = 'roc_auc', \n",
    "                                           return_train_score = True, n_jobs = -1)\n",
    "    random_search_obj.fit(x_train,y_train)\n",
    "    \n",
    "    print(f\"Mean_train_score is: {np.mean(random_search_obj.cv_results_['mean_train_score']):.3f}\")\n",
    "    print(f\"Mean_test_score is: {np.mean(random_search_obj.cv_results_['mean_test_score']):.3f}\")\n",
    "    print(f\"Best parameters are: {random_search_obj.best_params_}\")\n",
    "    \n",
    "    return random_search_obj.best_params_\n",
    "\n",
    "\n",
    "# Set the grid\n",
    "rf_grid = {\n",
    "    'n_estimators': range(105,125,5),\n",
    "    'max_depth': range(6,14,2),\n",
    "    'min_samples_split': range(4,10,2)\n",
    "}\n",
    "\n",
    "# Get the optimal parameters.\n",
    "rf_tuned_params = find_best_parameters_hypertune(trained_models['rf'], rf_grid, X_train, Y_train)\n",
    "\n",
    "# Tuned rf model\n",
    "tuned_rf_model = {\n",
    "\"rf\": RandomForestClassifier(**rf_tuned_params),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > 14) Create submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission_file(model, file_name, test_data):\n",
    "    \n",
    "    y_pred_final = model.predict(test_data)\n",
    "\n",
    "    submission_1 = submission.copy()\n",
    "    submission_1[\"ID\"] = test_data['ID']\n",
    "    submission_1[\"Interest_Rate\"] = y_pred_final\n",
    "    submission_1.to_csv(file_name+'.csv', index=False)\n",
    "    \n",
    "    return file_name + \" csv submission created successful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final = model_gbm.predict(df_test2, num_iteration=model_gbm.best_iteration)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
