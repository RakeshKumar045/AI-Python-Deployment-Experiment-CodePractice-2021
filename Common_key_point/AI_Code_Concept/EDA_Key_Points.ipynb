{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from imblearn.combine import SMOTETomek # over sampling method 1\n",
    "\n",
    "## RandomOverSampler to handle imbalanced data\n",
    "from imblearn.over_sampling import RandomOverSampler # over sampling method 2\n",
    "\n",
    "\n",
    "#Shuffle the dataframe\n",
    "# from sklearn.utils import shuffle # df1 = shuffle(df).reset_index(drop = True)\n",
    "\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "from pickle import dump, load\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from collections import Counter , defaultdict\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt, rcParams as rc\n",
    "\n",
    "%matplotlib inline\n",
    "rc[\"figure.figsize\"] = 10,6\n",
    "\n",
    "\n",
    "#Data Science and ML\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.feature_selection import RFE #Recursive Feature Selection\n",
    "import statsmodels.api as sm \n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, normalize, LabelEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green' >1) Dataset Understand, check & get detail, How much Missing values present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def understanding_dataset(dataset):\n",
    "    print(f\"Shape: {dataset.shape}\")\n",
    "    print(f\"Total Missing Value in Dataset: {dataset.isna().sum().sum()}\")\n",
    "    \n",
    "    for column in dataset.columns:\n",
    "        print(f\"===============Column: {column} ==============\")\n",
    "        print(f\"Number of unique values: {dataset[column].nunique()}\")\n",
    "        print(f\"Max: {dataset[column].max()}\")\n",
    "        print(f\"Min: {dataset[column].min()}\")\n",
    "        \n",
    "        if(dataset[column].isna().any()):\n",
    "            print(f\"Missing Value: {round((data[[column]].isna().sum() / len(data) ) * 100 , 2)}\")\n",
    "    \n",
    "        print(\"\\n\")\n",
    "\n",
    "def get_percentage_miss_value(dataset):\n",
    "    higher_miss_value_column = []\n",
    "    miss_threshold_value = 50\n",
    "    \n",
    "    for i in dataset.columns:\n",
    "        if dataset[i].isna().sum() > 1: \n",
    "            perectange_val = (dataset[i].isna().sum() / len(dataset)) * 100\n",
    "            print(\"Column-> \" , i, \", total no of missing value : \",dataset[i].isna().sum() , \" & :         \", round(perectange_val,2) ,\" %\")\n",
    "                \n",
    "            if(perectange_val > miss_threshold_value):\n",
    "                higher_miss_value_column.append(i)\n",
    "            \n",
    "    print(\"\\n\\n\\n\")\n",
    "    \n",
    "    if higher_miss_value_column:\n",
    "        print(\"Higher Missing values in Columns Name : \", higher_miss_value_column)\n",
    "    else:\n",
    "        print(\"There are no Higher Column Missing values in Dataset\")\n",
    "        \n",
    "            \n",
    "def check_null(dataframe, percentage=True):\n",
    "    if percentage==True:\n",
    "        return round(dataframe.isnull().mean()[dataframe.isnull().mean()>0].apply(lambda x: x*100),3)\n",
    "    else:\n",
    "        return dataframe.isnull().sum()[dataframe.isnull().sum()>0]\n",
    "    \n",
    "check_null(train_data, percentage=False)\n",
    "\n",
    "def remove_digit_numerical(dataset):\n",
    "    for i in dataset.columns:\n",
    "        dataset[i] = pd.to_numeric(dataset[i].str.replace(r\"[^\\d]\", \"\"))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cloumn_details_type_categorical(dataset):\n",
    "    for i in dataset.columns:\n",
    "        if(dataset[i].dtype == \"object\"):\n",
    "            print(\"Columns name :  \",i  )\n",
    "            \n",
    "            print(dict(Counter(dataset[i])))\n",
    "            \n",
    "            print(\"*\"*100)\n",
    "            print(\"\\n\")\n",
    "\n",
    "def check_cloumn_details_type_numberical(dataset):\n",
    "    for i in dataset.columns:\n",
    "        if (dataset[i].dtype == \"int\"):\n",
    "            print(\"Columns name :  \",i  )\n",
    "            \n",
    "            print(dict(Counter(dataset[i])))\n",
    "            print(\"*\"*100)\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            \n",
    "def check_cloumn_details_type_float(dataset):\n",
    "    for i in dataset.columns:\n",
    "        if (dataset[i].dtype == \"float\"):\n",
    "            print(\"Columns name :  \",i  )\n",
    "            \n",
    "            print(dict(Counter(dataset[i])))\n",
    "            print(\"*\"*100)\n",
    "            print(\"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_col(dataset): #testing has been pending\n",
    "    return dataset.select_dtypes(include=['object']) #get all categorical columns only\n",
    "\n",
    "def get_numercial_data_types_col_name(dataset):\n",
    "    return list(dataset.select_dtypes(include =  [\"int\" , \"float\"] ).columns)\n",
    "\n",
    "def convert_numerical_data_type(dataset):\n",
    "    col_list = list(dataset.select_dtypes(include =  [\"int\" , \"float\"] ).columns)\n",
    "    for numberical_colname in col_list:\n",
    "        dataset[numberical_colname] = pd.to_numeric(dataset[numberical_colname])  \n",
    "    return dataset\n",
    "    \n",
    "def get_column_integer_float_list(dataset):\n",
    "    numberical_int_columns = []\n",
    "    for i in dataset.columns:\n",
    "        if dataset[i].dtype == \"int\":\n",
    "            numberical_int_columns.append(i)\n",
    "        \n",
    "    numberical_float_columns = []\n",
    "    for i in dataset.columns:\n",
    "        if dataset[i].dtype == \"float\":\n",
    "            numberical_float_columns.append(i)\n",
    "            \n",
    "    return numberical_int_columns, numberical_float_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' >2) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='blue' >2.1)  Impute for missing value(Numberical : mean, categorical :mode(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameImputer(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0] if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "    \n",
    "df6 = DataFrameImputer().fit_transform(df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='red' >2.2)  Different Types of Encoding and convert categorial to numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convet_categorical_numerical_use_get_dummy(dataset):\n",
    "    df2_categorical = dataset.select_dtypes(include =  [\"object\"] )\n",
    "    df_dummies = pd.get_dummies(df2_categorical,drop_first=True)\n",
    "    \n",
    "    # drop categorical variables \n",
    "    dataset = dataset.drop(columns=list(df2_categorical.columns), axis=1)\n",
    "    \n",
    "    # concat dummy variables with X\n",
    "    dataset = pd.concat([dataset, df_dummies], axis=1)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "# We one-hot encode the categorical features\n",
    "def one_hot_encoding(rides):\n",
    "    dummy_fields = ['season', 'weather', 'month', 'hour', 'weekday']\n",
    "    for each in dummy_fields:\n",
    "        dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "        rides = pd.concat([rides, dummies], axis=1)\n",
    "    return rides\n",
    "\n",
    "\n",
    "#Label encoding\n",
    "def convert_to_numerical_label_encoding(dataset):\n",
    "    enc = LabelEncoder()\n",
    "    for i in dataset.columns:\n",
    "        if(dataset[i].dtype == \"object\"):\n",
    "            dataset[i] = enc.fit_transform(dataset[i])\n",
    "            \n",
    "    return dataset\n",
    "\n",
    "#function for converting categoric to num codes\n",
    "#Changing Categorical colum values to numeric codesÂ¶\n",
    "\n",
    "def cat_to_num(dataset):\n",
    "    for i in range(0, dataset.shape[1]):\n",
    "        #print(i)\n",
    "        if(dataset.iloc[:,i].dtypes == 'object'):\n",
    "            dataset.iloc[:,i] = pd.Categorical(dataset.iloc[:,i])\n",
    "            dataset.iloc[:,i] = dataset.iloc[:,i].cat.codes\n",
    "            dataset.iloc[:,i] = dataset.iloc[:,i].astype('object')\n",
    "    return dataset\n",
    "\n",
    "\n",
    "train = cat_to_num(train)\n",
    "test = cat_to_num(test)\n",
    "\n",
    "gender_encoder = OneHotEncoder(dtype=np.uint8)\n",
    "gender = gender_encoder.fit_transform(df[\"Gender\"].values.reshape(-1,1)).todense()\n",
    "\n",
    "X = np.array(np.hstack((X,cabs,life,dest,gender)))\n",
    "print (np.array(X),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeking only the numeric features from the data or selecting only categorical column\n",
    "numeric_features = df2.select_dtypes(include = [np.number] or [\"object\"] or [np.float])\n",
    "\n",
    "df['ExterQual'] = df.ExterQual.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='red' > 2.3) Scaling & Transform & Drop Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardscaler_preprocessing(dataset_train, dataset_test, num_col):\n",
    "    scaler = StandardScaler()\n",
    "   \n",
    "    dataset_train[num_col] = scaler.fit_transform(dataset_train[num_col])\n",
    "\n",
    "    dataset_test[num_col] = scaler.transform(dataset_test[num_col])\n",
    "    \n",
    "    return dataset_train, dataset_test\n",
    "\n",
    "def remove_outlier(dataframe):\n",
    "    low = .05\n",
    "    high = .95\n",
    "    quant_df = dataframe.quantile([low, high])\n",
    "    for name in list(dataframe.columns):\n",
    "        if is_numeric_dtype(dataframe[name]):\n",
    "            dataframe = dataframe[(dataframe[name] > quant_df.loc[low, name]) & (dataframe[name] < quant_df.loc[high, name])]\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def drop_outliers(dataset, columns_name):\n",
    "    for chk in columns_name:\n",
    "        \n",
    "#         plt.boxplot(dataset[chk]) # please with graph & it is optional\n",
    "        \n",
    "        Q1 = dataset[chk].quantile(.25)\n",
    "        Q3 = dataset[chk].quantile(.75)\n",
    "        IQR = Q3-Q1\n",
    "        dataset =  dataset[(dataset[chk] >= (Q1-(1.5*IQR))) & (dataset[chk] <= (Q3+(1.5*IQR)))] \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='red' > 2.3.b) Standard Scaler for Both C&R, but use only 1 or more or list integer cols only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_columns_int_list = [\"int_col_1\",\"int_col_2\", \"int_col_3\",\"int_col_4\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "transformer = ColumnTransformer([ (\"scaler\", scaler, scale_columns_int_list)])\n",
    "\n",
    "transform_X = transformer.fit_transform(X)\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(transform_X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='green' > 2.4) VIF : variance inflation feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using VIF for multicollinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "pd.Series([variance_inflation_factor(X_train.values, j) for j in range( X_train.shape[1])],index=X_train.columns)\n",
    "\n",
    "#or\n",
    "\n",
    "def calculate_vif(X_train_val): # x : \n",
    "    thresh=10.0\n",
    "    output= X_train_val\n",
    "    k=X_train_val.shape[1]\n",
    "    \n",
    "    vif=[variance_inflation_factor(X_train_val.values,j) for j in range(X_train_val.shape[1])]\n",
    "    for i in range(1,k):#till k cz atleast one column would be kept even if its greater than 5\n",
    "        print('iteration no',i)\n",
    "        print(vif)\n",
    "        a=np.argmax(vif)\n",
    "        print('max vif is for variable no',a)\n",
    "        print(\" \")\n",
    "        if vif[a]<=thresh:\n",
    "            break\n",
    "        else:\n",
    "            output=output.drop(output.columns[a],axis=1)\n",
    "            vif=[variance_inflation_factor(output.values,j) for j in range(output.shape[1])]\n",
    "    return output\n",
    "\n",
    "X_vif=calculate_vif(X_train)\n",
    "\n",
    "X_train_dup=X_vif\n",
    "X_train_dup.columns\n",
    "\n",
    "X_test= X_test[X_train_dup.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' >4) Imabalnced Dataset : Over Sampling(Majoruity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_imbalanced_dataset(dataset, target_col):\n",
    "    feature_columns = dataset.columns.tolist()\n",
    "    feature_columns = [c for c in feature_columns if c not in [target_col]]\n",
    "\n",
    "    X2_new = dataset[feature_columns]\n",
    "    Y2_new = dataset[target_col]\n",
    "\n",
    "    os =  RandomOverSampler(random_state=35)\n",
    "    X_feature_variables , y_output = os.fit_sample(X2_new, Y2_new)\n",
    "    \n",
    "    X_feature_variables[target_col] = y_output\n",
    "    \n",
    "    X_feature_variables = X_feature_variables.sample(frac = 1).reset_index(drop = True)\n",
    "    \n",
    "    return X_feature_variables\n",
    "\n",
    "Counter(df2.target)\n",
    "df3 = majority_imbalanced_dataset(df2.copy()   , \"target\")\n",
    "Counter(df3.target)\n",
    "\n",
    "\n",
    "# df4 = majority_imbalanced_dataset(df3.copy(), \"Response\")\n",
    "# print(len(df4[df4[\"Response\"] == 0]), len(df4[df4[\"Response\"] == 1]))\n",
    "# df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset['Origin'] = dataset['Origin'].map(lambda x: {1: 'USA', 2: 'Europe', 3: 'Japan'}.get(x))\n",
    "\n",
    "\n",
    "### Split the data into train and test\n",
    "\n",
    "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)\n",
    "\n",
    "\n",
    "sns.pairplot(train_dataset[[\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\"]], diag_kind=\"kde\")\n",
    "\n",
    "sns.pairplot(dataset, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe().transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
