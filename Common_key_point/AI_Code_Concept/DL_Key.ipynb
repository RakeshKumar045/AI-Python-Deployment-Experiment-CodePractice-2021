{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_shape or input_dim = df.shape[1] or [len(df.columns)]: column(no of columns) #doubts?\n",
    "\n",
    "#doubts? : df.shape[1] or X.shape[1](y not included)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter , defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# pd.set_option('display.max_columns', 30)\n",
    "from pandas import Series as s , DataFrame as df\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt, rcParams as rc\n",
    "%matplotlib inline\n",
    "rc[\"figure.figsize\"] = 10,6\n",
    "\n",
    "\n",
    "from scipy import stats as s\n",
    "from random import randint\n",
    "from re import search\n",
    "from pathlib import Path\n",
    "\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import pprint\n",
    "from pprint import pprint \n",
    "import re, pprint, random, requests, glob\n",
    "\n",
    "from scipy.stats import mstats\n",
    "from operator import itemgetter\n",
    "from timeit import timeit\n",
    "\n",
    "\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from imutils import paths\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import lime\n",
    "from lime import lime_image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='orange' > Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modle Selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "#Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, recall_score, precision_score, explained_variance_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='blue' > HighLevel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > Deep Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential, load_model, Model\n",
    "# from keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, AveragePooling2D, Input\n",
    "# from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "\n",
    "\n",
    "\n",
    "#note : tensorflow.keras(not working)\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "#OR\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dropout, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.layers import ZeroPadding2D, BatchNormalization, AveragePooling2D, Input, Lambda\n",
    "from tensorflow.keras.layers import Embedding,Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Dense, Activation, MaxPooling1D\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LambdaCallback\n",
    "\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "#VGG16\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 #OR\n",
    "from tensorflow.keras.applications import VGG16, VGG19 #best\n",
    "\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Set\n",
    "\n",
    "pd.options.display.max_columns = 250\n",
    "pd.options.display.max_rows = 2000\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Classification(Image) : Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(128, 128, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# the model so far outputs 3D feature maps (height, width, features)\n",
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "***************************************************************************************************\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape = (28, 28)))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "\n",
    "****************************************************************************************************\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape = [32, 32, 3]))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=2, padding='valid'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "************************************************************************************************\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same', kernel_initializer='he_uniform', input_shape = (img_width, img_height, 3)))\n",
    "model.add(MaxPool2D(2,2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#only for this particular code\n",
    "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Transfer Learning :  VGG19, Restnet50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restnet = ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3))\n",
    "\n",
    "output = restnet.layers[-1].output\n",
    "output = Flatten()(output)\n",
    "\n",
    "restnet = Model(restnet.input, outputs=output)\n",
    "for layer in restnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "restnet.summary(line_length=100)\n",
    "\n",
    "**************************************************************************************************\n",
    "vggModel = VGG19(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "outputs = vggModel.output\n",
    "outputs = Flatten(name=\"flatten\")(outputs)\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "outputs = Dense(2, activation=\"softmax\")(outputs)\n",
    "\n",
    "model_vgg19 = Model(inputs=vggModel.input, outputs=outputs)\n",
    "\n",
    "for layer in vggModel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_vgg19.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_vgg19.summary(line_length=100)\n",
    "\n",
    "\n",
    "\n",
    "*************************************************************************************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Transfer Learning : VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vggModel = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "outputs = vggModel.output\n",
    "outputs = Flatten(name=\"flatten\")(outputs)\n",
    "outputs = Dropout(0.5)(outputs)\n",
    "outputs = Dense(2, activation=\"softmax\")(outputs)\n",
    "\n",
    "model_vgg16 = Model(inputs=vggModel.input, outputs=outputs)\n",
    "\n",
    "for layer in vggModel.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_vgg16.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_vgg16\n",
    "model_vgg16.summary(line_length=100)\n",
    "\n",
    "************************************************************************************************************\n",
    "#VGG16 # from keras.applications.vgg16 import VGG16\n",
    "baseModel = VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False)\n",
    "\n",
    "for layer in baseModel.layers:\n",
    "  layer.trainable = False\n",
    "\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D()(headModel) #from keras.layers import AveragePooling2D\n",
    "headModel = Flatten()(headModel)\n",
    "headModel = Dense(128, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.2)(headModel)\n",
    "headModel = Dense(3, activation='softmax')(headModel)\n",
    "\n",
    "model = Model(inputs=baseModel.input, outputs=headModel) #from keras.models import Model\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\",loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,zoom_range = 0.2,rotation_range=15, horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(data_path + '/train',target_size = (224, 224),\n",
    "                                                 batch_size = 16,class_mode = 'categorical',shuffle=True)\n",
    "#Found 5144 images belonging to 3 classes.\n",
    "\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(data_path + '/test',target_size = (224, 224),\n",
    "                                            batch_size = 16,class_mode = 'categorical',shuffle = False)\n",
    "#Found 1288 images belonging to 3 classes.\n",
    "\n",
    "\n",
    "history = model.fit_generator(training_set,validation_data=test_set,epochs=12)\n",
    "\n",
    "acc, val_acc, loss, val_loss = history.history['accuracy'], history.history['val_accuracy'], history.history['loss'],history.history['val_loss']\n",
    "epochs=range(len(acc))\n",
    "\n",
    "#graph 1 for accuracy for each epoch\n",
    "\n",
    "plt.plot(epochs,acc,label='Trainin_acc',color='blue')\n",
    "plt.plot(epochs,val_acc,label='Validation_acc',color='red')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Accuracy\")\n",
    "\n",
    "#graph 1 for loss for each epoch\n",
    "plt.plot(epochs,loss,label='Training_loss',color='blue')\n",
    "plt.plot(epochs,val_loss,label='Validation_loss',color='red')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation loss\")\n",
    "\n",
    "\n",
    "*****************************************************************************************\n",
    "#VGG16\n",
    "# load the VGG16 network, ensuring the head FC layer sets are left\n",
    "# off\n",
    "baseModel = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "# construct the head of the model that will be placed on top of the\n",
    "# the base model\n",
    "headModel = baseModel.output\n",
    "headModel = AveragePooling2D(pool_size=(4, 4))(headModel)\n",
    "headModel = Flatten(name=\"flatten\")(headModel)\n",
    "headModel = Dense(64, activation=\"relu\")(headModel)\n",
    "headModel = Dropout(0.5)(headModel)\n",
    "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
    "# place the head FC model on top of the base model (this will become\n",
    "# the actual model we will train)\n",
    "model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "# loop over all layers in the base model and freeze them so they will\n",
    "# *not* be updated during the first training process\n",
    "for layer in baseModel.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5,verbose=1),\n",
    "            ModelCheckpoint(filepath=\"weights.h5\", monitor=\"val_acc\", mode='min', save_best_only=True,\n",
    "            save_weights_only=True,verbose=1)]\n",
    "\n",
    "#BS = 8\n",
    "history = model.fit_generator(trainAug.flow(trainX, trainY, batch_size=BS),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    validation_data=(testX, testY),\n",
    "    validation_steps=len(testX) // BS,\n",
    "    epochs=EPOCHS, callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Model Train, Save, Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',        optimizer='adam', metrics=['accuracy']) #binary classification\n",
    "model.compile(loss=\"categorical_crossentropy\",   optimizer=\"adam\", metrics=[\"accuracy\"]) #multi  classification\n",
    "\n",
    "# model.compile(optimizer='adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.compile(loss = 'mse', optimizer = \"adam\", metrics = ['mae', 'mse']) # Regression\n",
    "\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5,verbose=1),\n",
    "            ModelCheckpoint(filepath=\"weights.h5\", monitor=\"val_acc\", mode='min', save_best_only=True,\n",
    "            save_weights_only=True,verbose=1)]\n",
    "\n",
    "\n",
    "#image classification # train_set : x_train, y_train & test_set : x_test, y_test\n",
    "history = model.fit_generator(train_set, validation_data = test_set,batch_size = 70, epochs=12,callbacks=callbacks)\n",
    "\n",
    "\n",
    "#Non-Image Classification\n",
    "history = model.fit(X_train, y_train.to_numpy(), batch_size = 70, epochs = 5, verbose = 1, callbacks=callbacks) \n",
    "\n",
    "\n",
    "#Regression\n",
    "h=model.fit(X_train,y_train,epochs=1000,validation_split=0.2,verbose=0,callbacks=[tfdocs.modeling.EpochDots()])\n",
    "\n",
    "\n",
    "#Regression : The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "early_history=model.fit(X_train,Y_train,epochs=1000,validation_split=0.2,verbose=0,callbacks=[early_stop,tfdocs.modeling.EpochDots()])\n",
    "\n",
    "\n",
    "\n",
    "#BS = 8\n",
    "history = model.fit_generator(trainAug.flow(trainX, trainY, batch_size=BS),\n",
    "    steps_per_epoch=len(trainX) // BS,\n",
    "    validation_data=(testX, testY),\n",
    "    validation_steps=len(testX) // BS,\n",
    "    epochs=EPOCHS, callbacks=callbacks)\n",
    "\n",
    "\n",
    "\n",
    "model.evaluate(X_test, y_test.to_numpy()) #Non-Image Classification\n",
    "\n",
    "loss, mae, mse = model.evaluate(x_test, y_test, verbose=2) # Regression\n",
    "\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))\n",
    "\n",
    "test_predictions = model.predict(x_test).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP(Non-Image classification) : text(spam or not spam) \n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "confusion_matrix(np.argmax(y_test, 1), np.argmax(y_pred, 1))\n",
    "\n",
    "pd.crosstab(np.argmax(y_test, 1), np.argmax(y_pred, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"train_dl_model\"\n",
    "model.save_weights(file_name + '.h5')  # always save your weights after training or during training\n",
    "model.save( file_name + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs = 20)\n",
    "# history = model.fit(X_train, y_train, epochs = 10, batch_size = 10, validation_split=0.2)\n",
    "# history = model.fit(X_train, y_train, batch_size=10, epochs=10, verbose=1, validation_data=(X_test, y_test))\n",
    "# history.history\n",
    "\n",
    "train_gen, validation_gen = dataset_data_generator()\n",
    "\n",
    "def model_train_save(model,train_data, validation_data, file_name):\n",
    "    model.fit_generator(\n",
    "        train_data,\n",
    "        steps_per_epoch = 2000 // batch_size,\n",
    "        epochs = 5,\n",
    "        validation_data = validation_data,\n",
    "        validation_steps=800 // batch_size)\n",
    "    \n",
    "    model.save_weights(file_name + '.h5')  # always save your weights after training or during training\n",
    "    model.save( file_name + \".h5\")\n",
    "\n",
    "model_train_save(seq_model, train_gen, validation_gen , \"Cat_Dog_Model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='red' > V.V.I) Classification(Non-Image) : Model Building, Train, Evaluate dataset(Churn or Loan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(X.shape[1], activation='relu', input_dim = X.shape[1])) #X.shape[1] : column(no of columns)\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train.to_numpy(), batch_size = 70, epochs = 5, verbose = 1)\n",
    "\n",
    "\n",
    "y_pred = model.predict_classes(X_test)\n",
    "\n",
    "\n",
    "print(y_pred)\n",
    "print(y_test)\n",
    "\n",
    "\n",
    "model.evaluate(X_test, y_test.to_numpy())\n",
    "\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "# pd.crosstab(y_test, y_pred)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green' > V.V.I) a) Regression : Model Building, Train, Evaluate Dataset(MPG or price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/tensorflow/docs\n",
    "\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "\n",
    "\n",
    "def build_model_regression():\n",
    "    #input_shape or input_dim = train_dataset.shape[1] or [len(train_dataset.columns)]: column(no of columns) #doubts?\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.columns)]),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "\n",
    "model = build_model_regression()\n",
    "model.summary()\n",
    "\n",
    "\n",
    "EPOCHS = 1000\n",
    "\n",
    "history=model.fit(X_train,y_train,epochs=1000,validation_split=0.2,verbose=0,callbacks=[tfdocs.modeling.EpochDots()])\n",
    "\n",
    "\n",
    "\n",
    "#graph 1\n",
    "plotter = tfdocs.plots.HistoryPlotter(smoothing_std=2)\n",
    "plotter.plot({'Basic': history}, metric = \"mae\")\n",
    "plt.ylim([0, 10])\n",
    "plt.ylabel('MAE [MPG]') #mpg : target column name\n",
    "\n",
    "#graph 2\n",
    "plotter.plot({'Basic': history}, metric = \"mse\")\n",
    "plt.ylim([0, 20])\n",
    "plt.ylabel('MSE [MPG^2]')\n",
    "\n",
    "\n",
    "model = build_model_regression()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "early_history = model.fit(normed_train_data, train_labels, \n",
    "                    epochs=EPOCHS, validation_split = 0.2, verbose=0, \n",
    "                    callbacks=[early_stop, tfdocs.modeling.EpochDots()])\n",
    "\n",
    "#again write graph 1 and 2\n",
    "plotter.plot({'Early Stopping': early_history}, metric = \"mae\")\n",
    "plt.ylim([0, 10])\n",
    "plt.ylabel('MAE [MPG]')\n",
    "\n",
    "\n",
    "loss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))\n",
    "\n",
    "\n",
    "test_predictions = model.predict(normed_test_data).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='green' > V.V.I) b) Regression : Model Building, Train, Evaluate Dataset(Air Clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_data_generator():\n",
    "    batch_size = 16\n",
    "\n",
    "    # this is the augmentation configuration we will use for training \n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1. / 255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "    # OR\n",
    "    #train_datagen = ImageDataGenerator(rescale=1. / 255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "\n",
    "\n",
    "    # this is the augmentation configuration we will use for testing: only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# generator: read pictures found in,  subfolers of 'data/train', and indefinitely generate\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        'cats_dogs/train',  # this is the target directory\n",
    "        target_size=(128, 128),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        'cats_dogs/validation',\n",
    "        target_size=(128, 128),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "\n",
    "\n",
    "train_generator = datagen.flow_from_directory(directory=train_data_dir,\n",
    "                                              target_size = (32, 32),\n",
    "                                              classes = ['dogs', 'cats'],\n",
    "                                              class_mode = 'binary',\n",
    "                                              batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > Model Predict and Image Show , Resize using PIL \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_show_img(img_name, model):\n",
    "    \n",
    "    img = load_img(img_name)  # this is a PIL image\n",
    "    \n",
    "    x = img_to_array(img.resize([128, 128]))  # Numpy array : shape (3, 150, 150)\n",
    "    x = x.reshape((1,) + x.shape)  #  Numpy array : shape (1, 3, 150, 150)\n",
    "    \n",
    "    if (model.predict_classes(x) == 1):\n",
    "        print(\"It is a DOG\")\n",
    "    else:\n",
    "        print(\"It is a Cat\")\n",
    "    return img\n",
    "\n",
    "\n",
    "image_name = 'test_dog1.jpg'\n",
    "predict_model_show_img(image_name, seq_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='green' > Method 2 : (Evaluation) : Model Load and Predict and Image Show , Resize using PIL \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"cat_dog_model_1.h5\" \n",
    "image_name = 'test_cat3.jpg'\n",
    "\n",
    "def load_model_predict_show_img(model_name, img_name):\n",
    "    \n",
    "    img = load_img(img_name)  # this is a PIL image\n",
    "    \n",
    "    x = img_to_array(img.resize([128, 128]))  #Numpy array : shape (3, 150, 150)\n",
    "    x = x.reshape((1,) + x.shape)\n",
    "\n",
    "    model = load_model(model_name)\n",
    "    if (model.predict_classes(x) == 1):\n",
    "        print(\"It is a DOG\")\n",
    "    else:\n",
    "        print(\"It is a Cat\")\n",
    "\n",
    "    return img\n",
    "\n",
    "load_model_predict_show_img(model_name, img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_evaluation(model, X_test, y_test):\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "    y_pred = model.predict_classes(X_test)\n",
    "    \n",
    "    print(np.argmax(y_pred[0]))\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(\"model Accuracy : \", test_accuracy, \"Model Loss : \", test_loss)\n",
    "    print(\"model Accuracy 2nd way : \", accuracy)\n",
    "    \n",
    "    thresold_value = 0.5\n",
    "    \n",
    "    if accuracy > thresold_value:\n",
    "        print(\"Class 1\")\n",
    "    else:\n",
    "        print(\"Class 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the testing set\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predIdxs = model.predict(testX, batch_size=BS) #BS = 8\n",
    "# for each image in the testing set we need to find the index of the\n",
    "# label with corresponding largest predicted probability\n",
    "predIdxs = np.argmax(predIdxs, axis=1)\n",
    "# show a nicely formatted classification report\n",
    "print(classification_report(testY.argmax(axis=1), predIdxs, target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the confusion matrix and and use it to derive the raw\n",
    "# accuracy, sensitivity, and specificity\n",
    "cm = confusion_matrix(testY.argmax(axis=1), predIdxs)\n",
    "plot_confusion_matrix(cm, figsize=(4,8), hide_ticks=True, cmap=plt.cm.Blues)\n",
    "total = sum(sum(cm))\n",
    "acc = (cm[0, 0] + cm[1, 1]) / total\n",
    "sensitivity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "specificity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "# show the confusion matrix, accuracy, sensitivity, and specificity\n",
    "print(cm)\n",
    "print(\"acc: {:.4f}\".format(acc))\n",
    "print(\"sensitivity: {:.4f}\".format(sensitivity))\n",
    "print(\"specificity: {:.4f}\".format(specificity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > Image Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def img_show(X_train):\n",
    "    plt.figure()\n",
    "    plt.imshow(X_train[1])\n",
    "    plt.title(\"Title_Image_Name\")\n",
    "    plt.colorbar() #optional\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# history = model.fit(X_train, y_train, epochs = 10, batch_size = 10, validation_split=0.2)\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_learningCurve(history):\n",
    "  # Plot training & validation accuracy values\n",
    "  epoch_range = range(1, 6)\n",
    "  plt.plot(epoch_range, history.history['accuracy'])\n",
    "  plt.plot(epoch_range, history.history['val_accuracy'])\n",
    "  plt.title('Model accuracy')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "  # Plot training & validation loss values\n",
    "  plt.plot(epoch_range, history.history['loss'])\n",
    "  plt.plot(epoch_range, history.history['val_loss'])\n",
    "  plt.title('Model loss')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.legend(['Train', 'Val'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "plot_learningCurve(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = cv2.imread('../input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/COVID-1009.png')\n",
    "\n",
    "test_image = cv2.resize(test_image, (224,224),interpolation=cv2.INTER_NEAREST)\n",
    "plt.imshow(test_image)\n",
    "\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "probs = model.predict(test_image)\n",
    "pred_class = np.argmax(probs)\n",
    "\n",
    "class_dict = {0:'COVID19', 1:'NORMAL', 2:'PNEUMONIA'}\n",
    "pred_class = class_dict[pred_class]\n",
    "\n",
    "print('prediction: ',pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot training metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "N = EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on COVID-19 Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {\n",
    "    'family': 'Times New Roman',\n",
    "    'size': 12\n",
    "}\n",
    "matplotlib.rc('font', **font)\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "plot_confusion_matrix(conf_mat=mat, figsize=(12, 12), class_names = class_names, show_normed=False)\n",
    "\n",
    "#or \n",
    "class_names = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "plot_confusion_matrix(conf_mat=mat, figsize=(8, 8), class_names = class_names, show_normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update(matplotlib.rcParamsDefault)\n",
    "\n",
    "plt.rc('font',family='Serif')\n",
    "\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plot_confusion_matrix(conf_mat=mat, figsize=(6, 6), class_names = class_names, show_normed=False)\n",
    "plt.tight_layout()\n",
    "fig.savefig('confusion_matrix_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='red' >  Visaulization & Evalaution, Fit  for Classification(Non-Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"model_churn.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='acc', factor=0.2,patience=1, min_lr=0.0001)\n",
    "reduce_val_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.4, patience=1, min_lr=0.0001)\n",
    "es = EarlyStopping(monitor='loss', patience=3, verbose=1, mode='auto')\n",
    "\n",
    "callbacks_list = [es, reduce_lr, reduce_val_lr, checkpointer]\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=1026,#512, \n",
    "                    validation_split=0.2, verbose=2,#data=(X_val, y_val)\n",
    "                    callbacks=callbacks_list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def drow_roc_curve(y_true, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.title('ROC Curve - Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1],[0,1],'g--')\n",
    "    plt.xlim([-0.001, 1])\n",
    "    plt.ylim([0, 1.001])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def print_stats(ytest, ypred):\n",
    "    \n",
    "    print(\"Accuracy: {:.5f}, Cohen's Kappa Score: {:.5f}\".format(\n",
    "        accuracy_score(ytest, ypred), \n",
    "        cohen_kappa_score(ytest, ypred, weights=\"quadratic\")))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(ytest, ypred))\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(ytest, ypred))\n",
    "    \n",
    "    \n",
    "def drow_precision_recal(y_true, y_pred):\n",
    "    precision, recall, th = precision_recall_curve(y_true, y_pred)\n",
    "    plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n",
    "    plt.title('Recall vs Precision')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlim([-0.01, 0.5])\n",
    "    plt.show()\n",
    "    \n",
    "def drow_recall(y_true, y_pred):\n",
    "    precision, recall, th = precision_recall_curve(y_true, y_pred)\n",
    "    plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\n",
    "    plt.title('Recall for different threshold values')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.show()\n",
    "    \n",
    "def drow_precision(y_true, y_pred):\n",
    "    precision, recall, th = precision_recall_curve(y_true, y_pred)\n",
    "    plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\n",
    "    plt.title('Precision for different threshold values')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def drow_history_acc(history):\n",
    "    # list all data in history\n",
    "    print(history.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def drow_history_loss(history):\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
