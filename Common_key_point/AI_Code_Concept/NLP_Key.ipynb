{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > Common\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter , defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "from pandas import Series as s , DataFrame as df\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from matplotlib import pyplot as plt, rcParams as rc\n",
    "%matplotlib inline\n",
    "rc[\"figure.figsize\"] = 10,6\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from scipy import stats as s\n",
    "from random import randint\n",
    "from re import search\n",
    "from pathlib import Path\n",
    "\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import pprint\n",
    "from pprint import pprint \n",
    "import re, pprint, random, requests, glob\n",
    "\n",
    "from scipy.stats import mstats\n",
    "from operator import itemgetter\n",
    "from timeit import timeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='orange' > Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Modle Selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "#Evaluation Metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, recall_score, precision_score, explained_variance_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data Set\n",
    "\n",
    "pd.options.display.max_columns = 250\n",
    "pd.options.display.max_rows = 2000\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='blue' > HighLevel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='yellow' > NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import nltk, re, pprint, random\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.preprocessing.text import Tokenizer#break a string into tokens. \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,LSTM,Embedding\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models import word2vec\n",
    "\n",
    "\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "##vvi\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5, 'you': 6, 'am': 7, 'running': 8, 'with': 9}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!',\n",
    "    \"I am running with dog\"\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 1)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > NLP problem 1 : Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer() # or TfidfVectorizer()\n",
    "\n",
    "## let's get counts for the first 5 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:5])\n",
    "\n",
    "\n",
    "## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\n",
    "print(example_train_vectors[0].todense().shape)\n",
    "print(example_train_vectors[0].todense())\n",
    "\n",
    "\n",
    "train_vectors = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "\n",
    "## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n",
    "# that the tokens in the train vectors are the only ones mapped to the test vectors - \n",
    "# i.e. that the train and test vectors use the same set of tokens.\n",
    "test_vectors = count_vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "\n",
    "## Our vectors are really big, so we want to push our model's weights\n",
    "## toward 0 without completely discounting different words - ridge regression \n",
    "## is a good way to do this.\n",
    "import linear_model\n",
    "clf = linear_model.RidgeClassifier()\n",
    "\n",
    "scores = model_selection.cross_val_score(clf, train_vectors, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "scores\n",
    "\n",
    "\n",
    "clf.fit(train_vectors, train_df[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > NLP problem 2 : Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doc = nlp(\"Tea is healthy and calming, don't you think?\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)\n",
    "    \n",
    "print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")\n",
    "    \n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > V.V.I : Model Building, Train, Evaluate for NLP(text or spam, sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(train_data[\"Tweet\"].values)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "X = tokenizer.texts_to_sequences(train_data[\"Tweet\"].values)\n",
    "X = pad_sequences(X, maxlen=max_len, padding='post')\n",
    "\n",
    "#pre process test data with param of train data\n",
    "X_test = tokenizer.texts_to_sequences(test_data[\"Tweet\"].values)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len, padding='post')\n",
    "\n",
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, 300))\n",
    "for word, index in tqdm(tokenizer.word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "\n",
    "vector_features = 300\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, vector_features, input_length=X.shape[1], weights=[embedding_matrix], trainable=False))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(Bidirectional(LSTM(300, activation='relu', dropout=0.3, recurrent_dropout=0.3), input_shape=(vector_features, vocab_size)))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "\n",
    "#Model Accuracy\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "print(\"training acuuracy {0}% and training loss {1}%\".format(accuracy[-1]*100, loss[-1]*100))\n",
    "print(\"validation acuuracy {0}% and validation loss {1}%\".format(val_accuracy[-1]*100, val_loss[-1]*100))\n",
    "\n",
    "#plot \n",
    "\n",
    "plt.plot(accuracy,'g',label='training accuracy')\n",
    "plt.plot(val_accuracy, 'r', label='validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(loss,'g',label='training loss')\n",
    "plt.plot(val_loss, 'r', label='validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "confusion_matrix(np.argmax(y_test, 1), np.argmax(y_pred, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color ='blue' > create a BOW : bag of words model on the spam dataset (col : label(o/p), message(i/p))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "spam = pd.read_csv(\"SMSSpamCollection.txt\", sep = \"\\t\", names=[\"label\", \"message\"])\n",
    "\n",
    "\n",
    "# convert messages into list and all rows details converting in single list\n",
    "messages = [message for message in messages]\n",
    "print(messages)\n",
    "\n",
    "\n",
    "def preprocess(document):\n",
    "    'changes document to lower case and removes stopwords'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    print(\"document : \", document)\n",
    "    words = word_tokenize(document)\n",
    "    print(\"token : \", words)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document\n",
    "\n",
    "messages_preprocess = [preprocess(message) for message in messages]\n",
    "print(messages_preprocess)\n",
    "\n",
    "\n",
    "#### Creating bag of words model using count vectorizer function\n",
    "vectorizer = CountVectorizer() # or TfidfVectorizer()\n",
    "\n",
    "bow_model = vectorizer.fit_transform(documents)\n",
    "print(bow_model)  # returns the rown and column number of cells which have 1 as value\n",
    "\n",
    "\n",
    "print(bow_model.toarray()) # print the full sparse matrix\n",
    "\n",
    "\n",
    "print(bow_model.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "# look at the dataframe\n",
    "pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='red' > BOW : bag of words model on the spam dataset (col : label(o/p),msg(i/p)) & use Stemming & lemmatising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer = PorterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# add stemming and lemmatisation in the preprocess function\n",
    "def preprocess(document, stem=True):\n",
    "    'changes document to lower case and removes stopwords'\n",
    "\n",
    "    # change sentence to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenize into words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # remove stop words\n",
    "    words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "    \n",
    "    if stem:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    else:\n",
    "        words = [wordnet_lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "        #words = [wordnet_lemmatizer.lemmatize(word, pos='n') for word in words] # or\n",
    "\n",
    "    # join words to make sentence\n",
    "    document = \" \".join(words)\n",
    "    \n",
    "    return document\n",
    "\n",
    "\n",
    "messages = [preprocess(message, stem=True) for message in spam.message] # stem messages\n",
    "\n",
    "messages = [preprocess(message, stem=False) for message in spam.message] # lemmatise messages\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_word_frequency(messages, 10):\n",
    "\n",
    "\n",
    "# bag of words model\n",
    "vectorizer = CountVectorizer() # or TfidfVectorizer()\n",
    "bow_model = vectorizer.fit_transform(messages)\n",
    "\n",
    "# look at the dataframe\n",
    "pd.DataFrame(bow_model.toarray(), columns = vectorizer.get_feature_names())\n",
    "\n",
    "#output\n",
    "\n",
    "#Stemm :  359 tokens after lemmatizing the messages as compared to 381 tokens without stemming.\n",
    "\n",
    "\n",
    "#Lemm : 363 tokens after lemmatizing the messages as compared to 381 tokens without lemmatising. \n",
    "# But, on the other hand, stemmer reduces the token count to 359. Lemmatization doesn't work as expected \n",
    "# because the data is very unclean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lemmatising is faster than stemming in this case because the nltk lemmatiser also takes another argument called the part-of-speech (POS) tag of the input word.\n",
    "* The default part-of-speech tag is 'noun'..\n",
    "* Right now, the stemmer will have more accuracy than the lemmatiser because each word is lemmatised assuming it's a noun. To lemmatise efficiently, you need to pass it's POS tag manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "CPU times: user 207 µs, sys: 44 µs, total: 251 µs\n",
      "Wall time: 156 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='red' > Model Building(Train, Evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = message[:70] # Train : 70% of dataset\n",
    "X_test = message[70:] #  Test  : 30% of dataset\n",
    "\n",
    "y_train = label[:70] # Train : 70% of dataset\n",
    "y_test = label[70:] #  Test  : 30% of dataset\n",
    "\n",
    "vect = CountVectorizer(stop_words='english',max_features=5000)\n",
    "vect.fit(X_train)\n",
    "\n",
    "\n",
    "X_train_transformed = vect.transform(X_train) # train : fit & transform or fit_transform\n",
    "X_test_transformed  = vect.transform(X_test)  # test  : only transform\n",
    "\n",
    "\n",
    "bnb = BernoulliNB() # or MultinomialNB()\n",
    "\n",
    "\n",
    "bnb.fit(X_train_transformed,y_train)\n",
    "\n",
    "# predict class\n",
    "pred_train_ys = bnb.predict(X_train_transformed)\n",
    "pred_test_ys = bnb.predict(X_test)\n",
    "\n",
    "# accuracy\n",
    "print(\"Train accuracy: \", accuracy_score(y_train, pred_train_ys))\n",
    "\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, pred_test_ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MultinomialNB work very well & high performance in accuracy better than BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color ='red' > Word embeddings approach : 2 options(A.Use pre-trained word vectors (Glove), B.Train our own vectors) ----->      (Note : pre-trained word vectors (Glove) is better than Train our own vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green'   > Word embeddings approach :  A) Use pre-trained word vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_input_file = 'glove.6B.200d.txt'\n",
    "word2vec_output_file = 'glove.6B.200d.w2vformat.txt'\n",
    "\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"glove.6B.200d.w2vformat.txt\", binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence vector by averaging word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def sent_vec(sent):\n",
    "    wv_res = np.zeros(glove_model.vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in glove_model:\n",
    "            ctr += 1\n",
    "            wv_res += glove_model[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    #return (wv_res, ctr)\n",
    "    return wv_res\n",
    "\n",
    "\n",
    "train_doc_vecs = []\n",
    "for doc in X:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    train_doc_vecs.append(sent_vec(doc_words))\n",
    "    \n",
    "\n",
    "test_doc_vecs = []\n",
    "for doc in X_test:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    test_doc_vecs.append(sent_vec(doc_words))\n",
    "    \n",
    "### Building a predictive model on the averaged word vectors\n",
    "\n",
    "logreg = LogisticRegression(penalty=\"l1\", random_state=42, C = 3.5)\n",
    "logreg.fit(train_doc_vecs,y)\n",
    "\n",
    "pred_train_ys = logreg.predict(train_doc_vecs)\n",
    "pred_test_ys = logreg.predict(test_doc_vecs)\n",
    "print(\"Train accuracy: \", accuracy_score(pred_train_ys, y))\n",
    "print(\"Test accuracy: \", accuracy_score(pred_test_ys, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green'   > Word embeddings approach :  B) Train our own vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training our own wordvectors on the data\n",
    "# We'll create a combined text file to train our word vectors - more data is better. \n",
    "#Although in this case we would still have just 7.7K instances to learn from\n",
    "\n",
    "X_comb = np.concatenate([X,X_test])\n",
    "len(X_comb)\n",
    "print(X_comb[6000])\n",
    "\n",
    "\n",
    "w2v = word2vec.Word2Vec(X_comb, window=2, min_count=2, sg = 1, size=200)\n",
    "w2v.most_similar(\"future\", topn=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence vectors by averaging vectors for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vec_w2v(sent):\n",
    "    wv_res = np.zeros(w2v.vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in w2v:\n",
    "            ctr += 1\n",
    "            wv_res += w2v[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    return wv_res\n",
    "\n",
    "#### Getting the sentence vectors for the test and train sets\n",
    "\n",
    "train_doc_vecs = []\n",
    "for doc in X:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    train_doc_vecs.append(sent_vec_w2v(doc_words))\n",
    "    \n",
    "test_doc_vecs = []\n",
    "for doc in X_test:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    test_doc_vecs.append(sent_vec_w2v(doc_words))\n",
    "    \n",
    "logreg = LogisticRegression(penalty=\"l1\", random_state=42, C = 8)\n",
    "logreg.fit(train_doc_vecs,y)\n",
    "\n",
    "pred_train_ys = logreg.predict(train_doc_vecs)\n",
    "pred_test_ys = logreg.predict(test_doc_vecs)\n",
    "print(\"Train accuracy: \", accuracy_score(pred_train_ys, y))\n",
    "print(\"Test accuracy: \", accuracy_score(pred_test_ys, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color ='green' > plot_word_frequency graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_frequency(words, top_n=10):\n",
    "    word_freq = FreqDist(words)\n",
    "    labels = [element[0] for element in word_freq.most_common(top_n)]\n",
    "    counts = [element[1] for element in word_freq.most_common(top_n)]\n",
    "    plot = sns.barplot(labels, counts)\n",
    "    return plot\n",
    "\n",
    "\n",
    "\n",
    "plot_word_frequency(messages or document, 10): # i/p : message or doc or text , plot after preprocessing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
