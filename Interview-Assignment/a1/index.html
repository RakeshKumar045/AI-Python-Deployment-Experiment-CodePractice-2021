<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"/>
    <title>A1: N-Gram Language Models</title>
    <style>.markdown-preview:not([data-use-github-style]) { padding: 2em; font-size: 1.2em; color: rgb(171, 178, 191); background-color: rgb(40, 44, 52); overflow: auto; }
.markdown-preview:not([data-use-github-style]) > :first-child { margin-top: 0px; }
.markdown-preview:not([data-use-github-style]) h1, .markdown-preview:not([data-use-github-style]) h2, .markdown-preview:not([data-use-github-style]) h3, .markdown-preview:not([data-use-github-style]) h4, .markdown-preview:not([data-use-github-style]) h5, .markdown-preview:not([data-use-github-style]) h6 { line-height: 1.2; margin-top: 1.5em; margin-bottom: 0.5em; color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) h1 { font-size: 2.4em; font-weight: 300; }
.markdown-preview:not([data-use-github-style]) h2 { font-size: 1.8em; font-weight: 400; }
.markdown-preview:not([data-use-github-style]) h3 { font-size: 1.5em; font-weight: 500; }
.markdown-preview:not([data-use-github-style]) h4 { font-size: 1.2em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) h5 { font-size: 1.1em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) h6 { font-size: 1em; font-weight: 600; }
.markdown-preview:not([data-use-github-style]) strong { color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) del { color: rgb(124, 135, 156); }
.markdown-preview:not([data-use-github-style]) a, .markdown-preview:not([data-use-github-style]) a code { color: rgb(82, 139, 255); }
.markdown-preview:not([data-use-github-style]) img { max-width: 100%; }
.markdown-preview:not([data-use-github-style]) > p { margin-top: 0px; margin-bottom: 1.5em; }
.markdown-preview:not([data-use-github-style]) > ul, .markdown-preview:not([data-use-github-style]) > ol { margin-bottom: 1.5em; }
.markdown-preview:not([data-use-github-style]) blockquote { margin: 1.5em 0px; font-size: inherit; color: rgb(124, 135, 156); border-color: rgb(75, 83, 98); border-width: 4px; }
.markdown-preview:not([data-use-github-style]) hr { margin: 3em 0px; border-top: 2px dashed rgb(75, 83, 98); background: none; }
.markdown-preview:not([data-use-github-style]) table { margin: 1.5em 0px; }
.markdown-preview:not([data-use-github-style]) th { color: rgb(255, 255, 255); }
.markdown-preview:not([data-use-github-style]) th, .markdown-preview:not([data-use-github-style]) td { padding: 0.66em 1em; border: 1px solid rgb(75, 83, 98); }
.markdown-preview:not([data-use-github-style]) code { color: rgb(255, 255, 255); background-color: rgb(58, 63, 75); }
.markdown-preview:not([data-use-github-style]) pre.editor-colors { margin: 1.5em 0px; padding: 1em; font-size: 0.92em; border-radius: 3px; background-color: rgb(49, 54, 63); }
.markdown-preview:not([data-use-github-style]) kbd { color: rgb(255, 255, 255); border-width: 1px 1px 2px; border-style: solid; border-color: rgb(75, 83, 98) rgb(75, 83, 98) rgb(62, 68, 81); border-image: initial; background-color: rgb(58, 63, 75); }
.markdown-preview[data-use-github-style] { font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif; line-height: 1.6; word-wrap: break-word; padding: 30px; font-size: 16px; color: rgb(51, 51, 51); background-color: rgb(255, 255, 255); overflow: scroll; }
.markdown-preview[data-use-github-style] > :first-child { margin-top: 0px !important; }
.markdown-preview[data-use-github-style] > :last-child { margin-bottom: 0px !important; }
.markdown-preview[data-use-github-style] a:not([href]) { color: inherit; text-decoration: none; }
.markdown-preview[data-use-github-style] .absent { color: rgb(204, 0, 0); }
.markdown-preview[data-use-github-style] .anchor { position: absolute; top: 0px; left: 0px; display: block; padding-right: 6px; padding-left: 30px; margin-left: -30px; }
.markdown-preview[data-use-github-style] .anchor:focus { outline: none; }
.markdown-preview[data-use-github-style] h1, .markdown-preview[data-use-github-style] h2, .markdown-preview[data-use-github-style] h3, .markdown-preview[data-use-github-style] h4, .markdown-preview[data-use-github-style] h5, .markdown-preview[data-use-github-style] h6 { position: relative; margin-top: 1em; margin-bottom: 16px; font-weight: bold; line-height: 1.4; }
.markdown-preview[data-use-github-style] h1 .octicon-link, .markdown-preview[data-use-github-style] h2 .octicon-link, .markdown-preview[data-use-github-style] h3 .octicon-link, .markdown-preview[data-use-github-style] h4 .octicon-link, .markdown-preview[data-use-github-style] h5 .octicon-link, .markdown-preview[data-use-github-style] h6 .octicon-link { display: none; color: rgb(0, 0, 0); vertical-align: middle; }
.markdown-preview[data-use-github-style] h1:hover .anchor, .markdown-preview[data-use-github-style] h2:hover .anchor, .markdown-preview[data-use-github-style] h3:hover .anchor, .markdown-preview[data-use-github-style] h4:hover .anchor, .markdown-preview[data-use-github-style] h5:hover .anchor, .markdown-preview[data-use-github-style] h6:hover .anchor { padding-left: 8px; margin-left: -30px; text-decoration: none; }
.markdown-preview[data-use-github-style] h1:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h2:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h3:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h4:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h5:hover .anchor .octicon-link, .markdown-preview[data-use-github-style] h6:hover .anchor .octicon-link { display: inline-block; }
.markdown-preview[data-use-github-style] h1 tt, .markdown-preview[data-use-github-style] h2 tt, .markdown-preview[data-use-github-style] h3 tt, .markdown-preview[data-use-github-style] h4 tt, .markdown-preview[data-use-github-style] h5 tt, .markdown-preview[data-use-github-style] h6 tt, .markdown-preview[data-use-github-style] h1 code, .markdown-preview[data-use-github-style] h2 code, .markdown-preview[data-use-github-style] h3 code, .markdown-preview[data-use-github-style] h4 code, .markdown-preview[data-use-github-style] h5 code, .markdown-preview[data-use-github-style] h6 code { font-size: inherit; }
.markdown-preview[data-use-github-style] h1 { padding-bottom: 0.3em; font-size: 2.25em; line-height: 1.2; border-bottom: 1px solid rgb(238, 238, 238); }
.markdown-preview[data-use-github-style] h1 .anchor { line-height: 1; }
.markdown-preview[data-use-github-style] h2 { padding-bottom: 0.3em; font-size: 1.75em; line-height: 1.225; border-bottom: 1px solid rgb(238, 238, 238); }
.markdown-preview[data-use-github-style] h2 .anchor { line-height: 1; }
.markdown-preview[data-use-github-style] h3 { font-size: 1.5em; line-height: 1.43; }
.markdown-preview[data-use-github-style] h3 .anchor { line-height: 1.2; }
.markdown-preview[data-use-github-style] h4 { font-size: 1.25em; }
.markdown-preview[data-use-github-style] h4 .anchor { line-height: 1.2; }
.markdown-preview[data-use-github-style] h5 { font-size: 1em; }
.markdown-preview[data-use-github-style] h5 .anchor { line-height: 1.1; }
.markdown-preview[data-use-github-style] h6 { font-size: 1em; color: rgb(119, 119, 119); }
.markdown-preview[data-use-github-style] h6 .anchor { line-height: 1.1; }
.markdown-preview[data-use-github-style] p, .markdown-preview[data-use-github-style] blockquote, .markdown-preview[data-use-github-style] ul, .markdown-preview[data-use-github-style] ol, .markdown-preview[data-use-github-style] dl, .markdown-preview[data-use-github-style] table, .markdown-preview[data-use-github-style] pre { margin-top: 0px; margin-bottom: 16px; }
.markdown-preview[data-use-github-style] hr { height: 4px; padding: 0px; margin: 16px 0px; background-color: rgb(231, 231, 231); border: 0px none; }
.markdown-preview[data-use-github-style] ul, .markdown-preview[data-use-github-style] ol { padding-left: 2em; }
.markdown-preview[data-use-github-style] ul.no-list, .markdown-preview[data-use-github-style] ol.no-list { padding: 0px; list-style-type: none; }
.markdown-preview[data-use-github-style] ul ul, .markdown-preview[data-use-github-style] ul ol, .markdown-preview[data-use-github-style] ol ol, .markdown-preview[data-use-github-style] ol ul { margin-top: 0px; margin-bottom: 0px; }
.markdown-preview[data-use-github-style] li > p { margin-top: 16px; }
.markdown-preview[data-use-github-style] dl { padding: 0px; }
.markdown-preview[data-use-github-style] dl dt { padding: 0px; margin-top: 16px; font-size: 1em; font-style: italic; font-weight: bold; }
.markdown-preview[data-use-github-style] dl dd { padding: 0px 16px; margin-bottom: 16px; }
.markdown-preview[data-use-github-style] blockquote { padding: 0px 15px; color: rgb(119, 119, 119); border-left: 4px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] blockquote > :first-child { margin-top: 0px; }
.markdown-preview[data-use-github-style] blockquote > :last-child { margin-bottom: 0px; }
.markdown-preview[data-use-github-style] table { display: block; width: 100%; overflow: auto; word-break: keep-all; }
.markdown-preview[data-use-github-style] table th { font-weight: bold; }
.markdown-preview[data-use-github-style] table th, .markdown-preview[data-use-github-style] table td { padding: 6px 13px; border: 1px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] table tr { background-color: rgb(255, 255, 255); border-top: 1px solid rgb(204, 204, 204); }
.markdown-preview[data-use-github-style] table tr:nth-child(2n) { background-color: rgb(248, 248, 248); }
.markdown-preview[data-use-github-style] img { max-width: 100%; box-sizing: border-box; }
.markdown-preview[data-use-github-style] .emoji { max-width: none; }
.markdown-preview[data-use-github-style] span.frame { display: block; overflow: hidden; }
.markdown-preview[data-use-github-style] span.frame > span { display: block; float: left; width: auto; padding: 7px; margin: 13px 0px 0px; overflow: hidden; border: 1px solid rgb(221, 221, 221); }
.markdown-preview[data-use-github-style] span.frame span img { display: block; float: left; }
.markdown-preview[data-use-github-style] span.frame span span { display: block; padding: 5px 0px 0px; clear: both; color: rgb(51, 51, 51); }
.markdown-preview[data-use-github-style] span.align-center { display: block; overflow: hidden; clear: both; }
.markdown-preview[data-use-github-style] span.align-center > span { display: block; margin: 13px auto 0px; overflow: hidden; text-align: center; }
.markdown-preview[data-use-github-style] span.align-center span img { margin: 0px auto; text-align: center; }
.markdown-preview[data-use-github-style] span.align-right { display: block; overflow: hidden; clear: both; }
.markdown-preview[data-use-github-style] span.align-right > span { display: block; margin: 13px 0px 0px; overflow: hidden; text-align: right; }
.markdown-preview[data-use-github-style] span.align-right span img { margin: 0px; text-align: right; }
.markdown-preview[data-use-github-style] span.float-left { display: block; float: left; margin-right: 13px; overflow: hidden; }
.markdown-preview[data-use-github-style] span.float-left span { margin: 13px 0px 0px; }
.markdown-preview[data-use-github-style] span.float-right { display: block; float: right; margin-left: 13px; overflow: hidden; }
.markdown-preview[data-use-github-style] span.float-right > span { display: block; margin: 13px auto 0px; overflow: hidden; text-align: right; }
.markdown-preview[data-use-github-style] code, .markdown-preview[data-use-github-style] tt { padding: 0.2em 0px; margin: 0px; font-size: 85%; background-color: rgba(0, 0, 0, 0.0392157); border-radius: 3px; }
.markdown-preview[data-use-github-style] code::before, .markdown-preview[data-use-github-style] tt::before, .markdown-preview[data-use-github-style] code::after, .markdown-preview[data-use-github-style] tt::after { letter-spacing: -0.2em; content: " "; }
.markdown-preview[data-use-github-style] code br, .markdown-preview[data-use-github-style] tt br { display: none; }
.markdown-preview[data-use-github-style] del code { text-decoration: inherit; }
.markdown-preview[data-use-github-style] pre > code { padding: 0px; margin: 0px; font-size: 100%; word-break: normal; white-space: pre; background: transparent; border: 0px; }
.markdown-preview[data-use-github-style] .highlight { margin-bottom: 16px; }
.markdown-preview[data-use-github-style] .highlight pre, .markdown-preview[data-use-github-style] pre { padding: 16px; overflow: auto; font-size: 85%; line-height: 1.45; background-color: rgb(247, 247, 247); border-radius: 3px; }
.markdown-preview[data-use-github-style] .highlight pre { margin-bottom: 0px; word-break: normal; }
.markdown-preview[data-use-github-style] pre { word-wrap: normal; }
.markdown-preview[data-use-github-style] pre code, .markdown-preview[data-use-github-style] pre tt { display: inline; max-width: initial; padding: 0px; margin: 0px; overflow: initial; line-height: inherit; word-wrap: normal; background-color: transparent; border: 0px; }
.markdown-preview[data-use-github-style] pre code::before, .markdown-preview[data-use-github-style] pre tt::before, .markdown-preview[data-use-github-style] pre code::after, .markdown-preview[data-use-github-style] pre tt::after { content: normal; }
.markdown-preview[data-use-github-style] kbd { display: inline-block; padding: 3px 5px; font-size: 11px; line-height: 10px; color: rgb(85, 85, 85); vertical-align: middle; background-color: rgb(252, 252, 252); border-width: 1px; border-style: solid; border-color: rgb(204, 204, 204) rgb(204, 204, 204) rgb(187, 187, 187); border-image: initial; border-radius: 3px; box-shadow: rgb(187, 187, 187) 0px -1px 0px inset; }
.markdown-preview[data-use-github-style] a { color: rgb(51, 122, 183); }
.markdown-preview[data-use-github-style] code { color: inherit; }
.markdown-preview[data-use-github-style] pre.editor-colors { padding: 0.8em 1em; margin-bottom: 1em; font-size: 0.85em; border-radius: 4px; overflow: auto; }
.scrollbars-visible-always .markdown-preview pre.editor-colors .vertical-scrollbar, .scrollbars-visible-always .markdown-preview pre.editor-colors .horizontal-scrollbar { visibility: hidden; }
.scrollbars-visible-always .markdown-preview pre.editor-colors:hover .vertical-scrollbar, .scrollbars-visible-always .markdown-preview pre.editor-colors:hover .horizontal-scrollbar { visibility: visible; }
.markdown-preview .task-list-item-checkbox { position: absolute; margin: 0.25em 0px 0px -1.4em; }
.bracket-matcher .region {
  border-bottom: 1px dotted lime;
  position: absolute;
}
.line-number.bracket-matcher {
  background-color: #777;
}

.conflict-resolved {
  background: #31363f;
}
.conflict-ours {
  background: rgba(0, 136, 255, 0.4);
}
.conflict-ours.cursor-line {
  background: rgba(0, 136, 255, 0.3);
}
.conflict-theirs {
  background: rgba(32, 182, 132, 0.4);
}
.conflict-theirs.cursor-line {
  background: rgba(32, 182, 132, 0.3);
}
.conflict-base {
  background: rgba(204, 133, 51, 0.4);
}
.conflict-base.cursor-line {
  background: rgba(204, 133, 51, 0.3);
}
.conflict-dirty {
  background: rgba(235, 221, 91, 0.4);
}
.conflict-dirty.cursor-line {
  background: rgba(235, 221, 91, 0.3);
}

.spell-check-misspelling .region {
  border-bottom: 2px dotted rgba(255, 51, 51, 0.75);
}
.spell-check-corrections {
  width: 25em !important;
}

@import 'languages/css';
pre.editor-colors {
  background-color: #282c34;
}
pre.editor-colors .line.cursor-line {
  background-color: rgba(153, 187, 255, 0.04);
}
pre.editor-colors .invisible {
  color: #abb2bf;
}
pre.editor-colors .cursor {
  border-left: 2px solid #528bff;
}
pre.editor-colors .selection .region {
  background-color: #3e4451;
}
pre.editor-colors .bracket-matcher .region {
  border-bottom: 1px solid #528bff;
  box-sizing: border-box;
}
pre.editor-colors .invisible-character {
  color: rgba(171, 178, 191, 0.15);
}
pre.editor-colors .indent-guide {
  color: rgba(171, 178, 191, 0.15);
}
pre.editor-colors .wrap-guide {
  background-color: rgba(171, 178, 191, 0.15);
}
pre.editor-colors .gutter .line-number {
  color: #636d83;
  -webkit-font-smoothing: antialiased;
}
pre.editor-colors .gutter .line-number.cursor-line {
  color: #abb2bf;
  background-color: #2c313a;
}
pre.editor-colors .gutter .line-number.cursor-line-no-selection {
  background-color: transparent;
}
pre.editor-colors .gutter .line-number .icon-right {
  color: #abb2bf;
}
pre.editor-colors .gutter:not(.git-diff-icon) .line-number.git-line-removed.git-line-removed::before {
  bottom: -3px;
}
pre.editor-colors .gutter:not(.git-diff-icon) .line-number.git-line-removed::after {
  content: "";
  position: absolute;
  left: 0px;
  bottom: 0px;
  width: 25px;
  border-bottom: 1px dotted rgba(224, 82, 82, 0.5);
  pointer-events: none;
}
pre.editor-colors .gutter .line-number.folded,
pre.editor-colors .gutter .line-number:after,
pre.editor-colors .fold-marker:after {
  color: #abb2bf;
}
.syntax--comment {
  color: #5c6370;
  font-style: italic;
}
.syntax--entity.syntax--name.syntax--type {
  color: #ffb366;
}
.syntax--entity.syntax--other.syntax--inherited-class {
  color: #c3a1f2;
}
.syntax--keyword {
  color: #ff66a8;
}
.syntax--keyword.syntax--control {
  color: #ff66a8;
}
.syntax--keyword.syntax--operator {
  color: #abb2bf;
}
.syntax--keyword.syntax--other.syntax--special-method {
  color: #61afef;
}
.syntax--keyword.syntax--other.syntax--unit {
  color: #ffb366;
}
.syntax--storage {
  color: #ff66a8;
}
.syntax--storage.syntax--type.syntax--annotation,
.syntax--storage.syntax--type.syntax--primitive {
  color: #ff66a8;
}
.syntax--storage.syntax--modifier.syntax--package,
.syntax--storage.syntax--modifier.syntax--import {
  color: #abb2bf;
}
.syntax--constant {
  color: #ffb366;
}
.syntax--constant.syntax--variable {
  color: #ffb366;
}
.syntax--constant.syntax--character.syntax--escape {
  color: #98c379;
}
.syntax--constant.syntax--numeric {
  color: #ffb366;
}
.syntax--constant.syntax--other.syntax--color {
  color: #98c379;
}
.syntax--constant.syntax--other.syntax--symbol {
  color: #98c379;
}
.syntax--variable {
  color: #ff66a8;
}
.syntax--variable.syntax--interpolation {
  color: #be5046;
}
.syntax--variable.syntax--parameter {
  color: #abb2bf;
}
.syntax--string {
  color: #c3a1f2;
}
.syntax--string.syntax--regexp {
  color: #98c379;
}
.syntax--string.syntax--regexp .syntax--source.syntax--ruby.syntax--embedded {
  color: #ffb366;
}
.syntax--string.syntax--other.syntax--link {
  color: #ff66a8;
}
.syntax--punctuation.syntax--definition.syntax--comment {
  color: #5c6370;
}
.syntax--punctuation.syntax--definition.syntax--method-parameters,
.syntax--punctuation.syntax--definition.syntax--function-parameters,
.syntax--punctuation.syntax--definition.syntax--parameters,
.syntax--punctuation.syntax--definition.syntax--separator,
.syntax--punctuation.syntax--definition.syntax--seperator,
.syntax--punctuation.syntax--definition.syntax--array {
  color: #abb2bf;
}
.syntax--punctuation.syntax--definition.syntax--heading,
.syntax--punctuation.syntax--definition.syntax--identity {
  color: #61afef;
}
.syntax--punctuation.syntax--definition.syntax--bold {
  color: #ffb366;
  font-weight: bold;
}
.syntax--punctuation.syntax--definition.syntax--italic {
  color: #ff66a8;
  font-style: italic;
}
.syntax--punctuation.syntax--section.syntax--embedded {
  color: #be5046;
}
.syntax--punctuation.syntax--section.syntax--method,
.syntax--punctuation.syntax--section.syntax--class,
.syntax--punctuation.syntax--section.syntax--inner-class {
  color: #abb2bf;
}
.syntax--support.syntax--class {
  color: #ffb366;
}
.syntax--support.syntax--type {
  color: #98c379;
}
.syntax--support.syntax--function {
  color: #98c379;
}
.syntax--support.syntax--function.syntax--any-method {
  color: #61afef;
}
.syntax--entity.syntax--name.syntax--function {
  color: #61afef;
}
.syntax--entity.syntax--name.syntax--class,
.syntax--entity.syntax--name.syntax--type.syntax--class {
  color: #ffb366;
}
.syntax--entity.syntax--name.syntax--section {
  color: #61afef;
}
.syntax--entity.syntax--name.syntax--tag {
  color: #ff66a8;
}
.syntax--entity.syntax--other.syntax--attribute-name {
  color: #ffb366;
}
.syntax--entity.syntax--other.syntax--attribute-name.syntax--id {
  color: #61afef;
}
.syntax--meta.syntax--class {
  color: #ffb366;
}
.syntax--meta.syntax--class.syntax--body {
  color: #abb2bf;
}
.syntax--meta.syntax--method-call,
.syntax--meta.syntax--method {
  color: #abb2bf;
}
.syntax--meta.syntax--definition.syntax--variable {
  color: #ff66a8;
}
.syntax--meta.syntax--link {
  color: #ffb366;
}
.syntax--meta.syntax--require {
  color: #61afef;
}
.syntax--meta.syntax--selector {
  color: #ff66a8;
}
.syntax--meta.syntax--separator {
  background-color: #373b41;
  color: #abb2bf;
}
.syntax--meta.syntax--tag {
  color: #abb2bf;
}
.syntax--none {
  color: #abb2bf;
}
.syntax--invalid.syntax--deprecated {
  color: #523d14 !important;
  background-color: #e0c285 !important;
}
.syntax--invalid.syntax--illegal {
  color: white !important;
  background-color: #e05252 !important;
}
.syntax--markup.syntax--bold {
  color: #ffb366;
  font-weight: bold;
}
.syntax--markup.syntax--changed {
  color: #ff66a8;
}
.syntax--markup.syntax--deleted {
  color: #ff66a8;
}
.syntax--markup.syntax--italic {
  color: #ff66a8;
  font-style: italic;
}
.syntax--markup.syntax--heading {
  color: #ff66a8;
}
.syntax--markup.syntax--heading .syntax--punctuation.syntax--definition.syntax--heading {
  color: #61afef;
}
.syntax--markup.syntax--link {
  color: #ff66a8;
}
.syntax--markup.syntax--inserted {
  color: #c3a1f2;
}
.syntax--markup.syntax--quote {
  color: #ffb366;
}
.syntax--markup.syntax--raw {
  color: #c3a1f2;
}
.syntax--source.syntax--cs .syntax--keyword.syntax--operator {
  color: #ff66a8;
}
.syntax--source.syntax--gfm .syntax--markup {
  -webkit-font-smoothing: auto;
}
.syntax--source.syntax--gfm .syntax--link .syntax--entity {
  color: #61afef;
}
.syntax--source.syntax--ini .syntax--keyword.syntax--other.syntax--definition.syntax--ini {
  color: #ff66a8;
}
.syntax--source.syntax--java .syntax--storage.syntax--modifier.syntax--import {
  color: #ffb366;
}
.syntax--source.syntax--java .syntax--storage.syntax--type {
  color: #ffb366;
}
.syntax--source.syntax--java-properties .syntax--meta.syntax--key-pair {
  color: #ff66a8;
}
.syntax--source.syntax--java-properties .syntax--meta.syntax--key-pair > .syntax--punctuation {
  color: #abb2bf;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--string.syntax--quoted.syntax--json {
  color: #ff66a8;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--string.syntax--quoted.syntax--json > .syntax--punctuation.syntax--string {
  color: #ff66a8;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--array.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json > .syntax--punctuation,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--array.syntax--json > .syntax--value.syntax--json > .syntax--string.syntax--quoted.syntax--json > .syntax--punctuation {
  color: #c3a1f2;
}
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--dictionary.syntax--json > .syntax--constant.syntax--language.syntax--json,
.syntax--source.syntax--json .syntax--meta.syntax--structure.syntax--array.syntax--json > .syntax--constant.syntax--language.syntax--json {
  color: #98c379;
}
.syntax--source.syntax--ruby .syntax--constant.syntax--other.syntax--symbol > .syntax--punctuation {
  color: inherit;
}
.syntax--source.syntax--python .syntax--keyword.syntax--operator.syntax--logical.syntax--python {
  color: #ff66a8;
}
.syntax--source.syntax--python .syntax--variable {
  color: #ffb366;
  font-style: italic;
}
.syntax--source.syntax--python .syntax--variable.syntax--parameter {
  color: #ffb366;
  font-style: normal;
}

    </style>
</head>
<body class='markdown-preview' data-use-github-style><h1 id="a1-n-gram-language-models">A1: N-Gram Language Models</h1>
<p><em>Authors: Klinton Bicknell, Harry Eldridge, Nathan Schneider</em></p>
<p><em>Turning it in.</em> Upload a .zip file of all your code, output files, and written responses to Canvas</p>
<p><em>Starter code:</em> <a href="a1.zip">a1.zip</a></p>
<p><em>Grading:</em> The assignment is worth 60 points, distributed as follows: Q1: 5, Q2: 10 (5 code, 5 written), Q3:
    9, Q4: 10 (5 code, 5 written),
    Q5: 9, Q6: 12 (8 code, 4 written), Q7: 5.</p>
<h2 id="unigram-model">Unigram model</h2>
<h3 id="1-creating-the-word_to_index-dictionary">1. Creating the <code>word_to_index</code> dictionary</h3>
<p>[Coding only: use starter code <a href="problem1.py">problem1.py</a>]</p>
<p>The first step in building an n-gram
    model is to create a dictionary that maps words to indices (which we’ll
    use to access the elements corresponding to that word in a vector or
    matrix of counts or probabilities). You’ll create this dictionary from a
    vocabulary file that lists each word in the corpus exactly once: <a
            href="brown_vocab_100.txt">brown_vocab_100.txt</a></p>
<p>This file lists the 811 word types used in the first 100 sentences of
    the Brown corpus and includes the special word types <code>&lt;s&gt;</code> and <code>&lt;/s&gt;</code>,
    for a total of 813 word types. In this version of the Brown corpus,
    punctuation marks are also treated as words, so they are also included
    in this total. In the corpus itself, punctuation words are separated
    from other words by a space, just like other words are, so you won’t
    need to treat them differently at all. The <a href="brown_vocab_100.txt">brown_vocab_100.txt</a> file is
    formatted to contain one word per line. We want to create a dictionary
    in python that maps each word to its order of occurrence in this file,
    starting with 0. For example, the first word in the file is ‘all’ and
    this should map to 0. The last word is ‘resolution’ and this should map
    to 812. To create the dictionary in python, you’ll iterate over each
    line (which is a word plus a newline character), use <code>rstrip()</code> to
    remove the trailing newline character, and then add the word to the
    dictionary mapped to its appropriate index. To do this, you’ll need to
    keep track of which line of the file you’re in using <code>enumerate()</code>.</p>
<p>After creating this dictionary, write the dictionary to a file called
    <code>word_to_index_100.txt</code>. Because <code>wf.write()</code> only writes strings, you
    will need to convert the dictionary to a string before you can write it
    using the <code>str()</code> function. To check that you’ve done this correctly,
    verify in the file output that ‘all’ is mapped to 0 and ‘resolution’ to
    812.</p>
<h3 id="2-building-an-mle-unigram-model">2. Building an MLE unigram model</h3>
<p>[Coding and written answer: use starter code <a href="problem2.py">problem2.py</a>]</p>
<p>Now you’ll build a simple MLE unigram model from the first 100 sentences
    in the Brown corpus, found in: <a href="brown_100.txt">brown_100.txt</a></p>
<p>This corpus is represented as one sentence per line with a space
    separating all words, as well as the end-of-sentence word <code>&lt;/s&gt;</code>. You’ll need to split the
    sentences into a list of
    words (e.g., using the string’s <code>.split()</code> member function) and convert
    each word to lowercase (e.g., using the string’s <code>.lower()</code> member
    function).</p>
<p>First, copy your code from problem 1 to load the dictionary mapping
    words to indices. Then <code>import numpy as np</code> and initialize the numpy
    vector of <code>counts</code> with zeros. Finally, iterate through the sentences
    and increment counts for each of the words they contain.</p>
<p>Now print the <code>counts</code> vector. (Do this in the python interpreter. Don’t
    add this printing to your script.) <em>Q: Estimate (just by eyeballing) the
        proportion of the word types that occurred only once in this corpus. Do
        you think the proportion of words that occur only once would be higher
        or lower if we used a larger corpus (e.g., all 57000 sentences in
        Brown)? Why or why not?</em></p>
<p>Finally, to normalize your counts vector and create probabilities, you
    can simply divide the counts vector by its sum in numpy like so:</p>
<pre class="editor-colors lang-py"><div class="line"><span class="syntax--source syntax--python"><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>probs</span><span>&nbsp;</span><span
        class="syntax--keyword syntax--operator syntax--assignment syntax--python"><span>=</span></span><span>&nbsp;</span><span>counts</span><span>&nbsp;</span><span
        class="syntax--keyword syntax--operator syntax--arithmetic syntax--python"><span>/</span></span><span>&nbsp;</span><span
        class="syntax--meta syntax--function-call syntax--python"><span>np</span><span>.</span><span>sum</span><span
        class="syntax--punctuation syntax--definition syntax--arguments syntax--begin syntax--python"><span>(</span></span><span
        class="syntax--meta syntax--function-call syntax--arguments syntax--python"><span>counts</span></span><span
        class="syntax--punctuation syntax--definition syntax--arguments syntax--end syntax--python"><span>)</span></span></span></span></div></pre>
<p>Write your new <code>probs</code> vector to a file called <code>unigram_probs.txt</code> and
    verify that the first probability in it (the probability of ‘all’) is
    0.00040519 and that the last probability (probability of ‘resolution’) is
    0.00364668. (Note that our model trained on this small corpus has
    estimated that ‘resolution’ is about 10 times as frequent as ‘all’!
    Models trained on very small corpora are very noisy.)</p>
<h2 id="bigram-models">Bigram models</h2>
<h3 id="3-building-an-mle-bigram-model">3. Building an MLE bigram model</h3>
<p>[Coding only: use starter code <a href="problem3.py">problem3.py</a>]</p>
<p>Now, you’ll create an MLE bigram model, in much the same way as you
    created an MLE unigram model. I recommend writing the code again from
    scratch, however (except for the code initializing the mapping
    dictionary), so that you can test things as you go. The main differences
    between coding an MLE bigram model and a unigram model are:</p>
<ul>
    <li>you’ll initialize a numpy matrix instead of a numpy vector</li>
    <li>you’ll increment counts for a combination of word and previous word.
        This means you’ll need to keep track of what the previous word was.
        I recommend doing this by (1) initializing <code>previous_word = '&lt;s&gt;'</code>
        just prior to the <code>for</code> loop iterating through a line (skipping the <code>'&lt;s&gt;'</code> at the
        0th index), and then (2)
        inside the <code>for</code> loop, adding a line that sets
        <code>previous_word = word</code> <em>after</em> incrementing the counts for the
        current <code>previous_word, word</code> pair. Think this through to make sure
        you understand why this ensures that the variable <code>previous_word</code>
        will always be the appropriate previous word when incrementing
        counts.
    </li>
    <li><p>you’ll now normalize each row of the counts matrix instead of just
        normalizing a single counts vector. To do that, replace the line
        involving <code>np.sum()</code> with the lines</p>
        <pre class="editor-colors lang-py"><div class="line"><span class="syntax--source syntax--python"><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span
                class="syntax--keyword syntax--control syntax--import syntax--from syntax--python"><span>from</span></span><span>&nbsp;</span><span>sklearn</span><span>.</span><span>preprocessing</span><span>&nbsp;</span><span
                class="syntax--keyword syntax--control syntax--import syntax--python"><span>import</span></span><span>&nbsp;</span><span>normalize</span></span></div><div
                class="line"><span class="syntax--source syntax--python"><span>&nbsp;&nbsp;&nbsp;&nbsp;</span><span>probs</span><span>&nbsp;</span><span
                class="syntax--keyword syntax--operator syntax--assignment syntax--python"><span>=</span></span><span>&nbsp;</span><span
                class="syntax--meta syntax--function-call syntax--python"><span>normalize</span><span
                class="syntax--punctuation syntax--definition syntax--arguments syntax--begin syntax--python"><span>(</span></span><span
                class="syntax--meta syntax--function-call syntax--arguments syntax--python"><span>counts</span><span>,&nbsp;</span><span
                class="syntax--variable syntax--parameter syntax--function syntax--python"><span>norm</span></span><span
                class="syntax--keyword syntax--operator syntax--assignment syntax--python"><span>=</span></span><span
                class="syntax--string syntax--quoted syntax--single syntax--single-line syntax--python"><span
                class="syntax--punctuation syntax--definition syntax--string syntax--begin syntax--python"><span>'</span></span><span>l1</span><span
                class="syntax--punctuation syntax--definition syntax--string syntax--end syntax--python"><span>'</span></span></span><span
                class="syntax--punctuation syntax--separator syntax--parameters syntax--python"><span>,</span></span><span>&nbsp;</span><span
                class="syntax--variable syntax--parameter syntax--function syntax--python"><span>axis</span></span><span
                class="syntax--keyword syntax--operator syntax--assignment syntax--python"><span>=</span></span><span
                class="syntax--constant syntax--numeric syntax--integer syntax--decimal syntax--python"><span>1</span></span></span><span
                class="syntax--punctuation syntax--definition syntax--arguments syntax--end syntax--python"><span>)</span></span></span></span></div></pre>
    </li>
    <li><p>Make sure the matrix is arranged so each row represents the probability of a word given the
        seen word so that it works with the generator. For more on this see <a href="generate.py">generate.py</a></p>
    </li>
</ul>
<p>When complete, add code to write the following probabilities to
    <code>bigram_probs.txt</code>, one per line:</p>
<ul>
    <li>p(the | all)</li>
    <li>p(jury | the)</li>
    <li>p(campaign | the)</li>
    <li>p(calls | anonymous)</li>
</ul>
<p>Make sure that the first two of these probabilities are 1.0 and about
    .08333.</p>
<h3 id="4-add-smoothing-the-bigram-model">4. Add-α smoothing the bigram model</h3>
<p>[Coding and written answer: save code as <code>problem4.py</code>]</p>
<p>This time, copy <code>problem3.py</code> to <code>problem4.py</code>. We’ll just be making a
    very small modification to the program to add smoothing. In class, we
    discussed two types of smoothing in detail: Laplace smoothing, in which
    1 <em>pseudocount</em> is added to every bigram count, and add-α smoothing, in which some amount α is added to every
    bigram count.
    (Laplace smoothing is thus a special case of add-α smoothing.)</p>
<p>You should modify your problem to use add-α smoothing with α, i.e., pretending that we saw an
    extra one-tenth of an instance of each bigram. With numpy, you can very
    simply add 0.1 to every cell of a matrix like so:</p>
<pre class="editor-colors lang-py"><div class="line"><span
        class="syntax--source syntax--python"><span>&nbsp;&nbsp;</span><span>counts</span><span>&nbsp;</span><span
        class="syntax--keyword syntax--operator syntax--assignment syntax--augmented syntax--python"><span>+=</span></span><span>&nbsp;</span><span
        class="syntax--constant syntax--numeric syntax--float syntax--python"><span>0.1</span></span></span></div></pre>
<p>Now also change the program to write the same four probabilities as
    before to a file called <code>smooth_probs.txt</code>. To verify that your program
    is working correctly, check that the first probability in the file is
    about 0.0133657.</p>
<p>Finally, compare <code>smooth_probs.txt</code> to <code>bigram_probs.txt</code>. <em>Q: Why did
    all four probabilities go down in the smoothed model?</em> Now note that the
    probabilities did not all decrease by the same amount. In particular,
    the two probabilities conditioned on ‘the’ dropped only slightly, while
    the other two probabilities (conditioned on ‘all’ and ‘anonymous’)
    dropped rather dramatically. <em>Q: Why did add-α smoothing cause probabilities
        conditioned on ‘the’ to fall much less than these others? And why is
        this behavior (causing probabilities conditioned on ‘the’ to fall less
        than the others) a good thing?</em> In figuring this out, you may find it
    useful to look at the relevant individual rows of the counts matrix
    (prior to adding the 0.1) to see how they’re different. In numpy, you
    can look at nth row of the <code>counts</code>
    matrix using <code>counts[n,]</code>.</p>
<h2 id="using-n-gram-models">Using n-gram models</h2>
<h3 id="5-experimenting-with-a-mle-trigram-model">5. Experimenting with a MLE trigram model</h3>
<p>[Coding only: save code as <code>problem5.py</code>]</p>
<p>Using your knowledge of language models, compute what the following probabilities would be in both
    a smoothed and unsmoothed trigram model (note, you should not be building an entire model, just what you need to
    calculate these probabilities):</p>
<ul>
    <li>p(past | in, the) (should be 0.0625 for unsmoothed, and ~0.011305 for smoothed)</li>
    <li><p>p(time | in, the)</p>
    </li>
    <li><p>p(said | the, jury)</p>
    </li>
    <li><p>p(recommended | the, jury)</p>
    </li>
    <li><p>p(that | jury, said)</p>
    </li>
    <li>p(, | agriculture, teacher)</li>
</ul>
<h3 id="6-calculating-sentence-probabilities">6. Calculating sentence probabilities</h3>
<p>[Coding and written answer]</p>
<p>For this problem, you will use each of the three models you’ve
    constructed in problems 2–4 to evaluate the probability of a toy corpus
    of sentences (containing just the first two sentences of the Brown corpus) found at: <a href="toy_corpus.txt">toy_corpus.txt</a>
</p>
<p>The sentences are contained in this corpus in the same format as the
    other corpora you’ve been using so far: one sentence per line and words
    separated by a space. As before, you’ll need to remove the end-of-line
    character.</p>
<p>First, you’ll edit <code>problem2.py</code>, and add code at the bottom of the
    script to iterate through each sentence in the toy corpus and calculate
    the joint probability of all the words in the sentence under the unigram
    model. Then write the probability of each sentence to a file
    <code>unigram_eval.txt</code>, formatted to have one probability for each line of
    the output file. To do this, you’ll be writing a loop within a loop very
    similarly to how you iterated through the training corpus to estimate
    the model parameters. But instead of incrementing counts after each
    word, you’ll be updating the joint probability of the sentence
    (multiplying the probabilities of each word together). One easy way to
    do this is to initialize <code>sentprob = 1</code> prior to looping through the
    words in the sentence, and then just update <code>sentprob *= wordprob</code> with
    the probability of each word. At the end of the loop, <code>sentprob</code> will
    contain the total joint probability of the whole sentence. To verify
    that this worked correctly, note that the joint probability of the
    second sentence under this model should be 4.84008387782e-99</p>
<p>Next, you’ll transform this joint probability into a perplexity (of each
    sentence), and write that to the file instead. To calculate the
    perplexity, first calculate the length of the sentence in words (be sure
    to include the end-of-sentence word) and store that in a variable
    <code>sent_len</code>, and then you can calculate
    <code>perplexity = 1/(pow(sentprob, 1.0/sent_len))</code>, which reproduces the
    definition of perplexity we discussed in class. Now, write the
    perplexity of each sentence to the output file instead of the joint
    probabilities. To verify that you’ve done this correctly, note that the
    perplexity of the second sentence with this model should be about 153.</p>
<p>Now, you’ll do the same thing for your other two models. Add code to
    <code>problem3.py</code> to calculate the perplexities of each sentence in the toy
    corpus and write that to a file <code>bigram_eval.txt</code>. Similarly, add code
    to <code>problem4.py</code> and write the perplexities under the smoothed model to
    <code>smoothed_eval.txt</code>. To verify that you did these correctly, note that
    the perplexity of the second sentence should be about 7.57 with the MLE
    bigram model and about 54.28 for the smoothed bigram model.
    Note when calculating perplexity that the number of bigrams is different from the number of tokens.</p>
<!-- fixed buggy numbers, added hint -->
<p>Compare the perplexities of these two sentences under all three models.
    <em>Q: Which model performed worst and why might you have expected that
        model to have performed worst?</em>
    <em>Q: Did smoothing help or hurt the model’s ‘performance’ when evaluated on this
        corpus? Why might that be?</em></p>
<h3 id="7-generation">7. Generation</h3>
<p>[Written answer]</p>
<p>Edit problem2-4.py to use the provided generator code to generate some (~10) sentences from each of your models.
    Store the sentences in <code>unigram_generation.txt</code>, <code>bigram_generation.txt</code>, and <code>smoothed_generation.txt</code>
    to go with their respective models.</p>
<p><em>Q: Compare the models qualitatively based on their generation. How does each perform?</em></p></body>
</html>
